# VLM基盤移行・最終改善提案書 (Qwen2-VL-2B)
**作成日:** 2026-01-04
**対象:** Kotaro-Engine V2.3 (on LMDeploy / Qwen2-VL-2B)

## 1. 目的と戦略
「VRAM 8GB制限」と「虎太郎人格の再現性（ブレないコメント）」を両立させるため、以下の戦略を採用する。

### 戦略的転換
- **従前案:** モデル分離（画像用AI ＋ 文書用AI）
  - 問題点: VRAM 8GB環境では2つのモデルをロードするとメモリ不足でクラッシュ・激重になる。
- **採用案:** **「役割分離（2パス・アーキテクチャ）」**
  - モデルは `Qwen2-VL-2B-Instruct` 1つのみを使用（省メモリ・高速）。
  - 処理を「工程」で分けることで、実質的に2系統と同等の品質担保を行う。

## 2. 2パス・アーキテクチャ詳細

### Pass A: 画像分析フェーズ (The "Eye")
- **入力:** 画像データ
- **出力:** 構造化JSONデータ (`docs/kotaro_vlm_schema_v2.json` 準拠)
- **役割:** 
  - 画像から客観的な事実（構図、表情、色、アイテム）を抽出する。
  - **虎太郎の人格（口調、感情）は一切排除**し、評価のみに徹する。
  - 60項目の判定基準 (Criteria) を `0/1` で判定する。

### Pass B: 人格生成フェーズ (The "Soul")
- **入力:** Pass A で生成されたJSONデータ
- **出力:** 虎太郎コメント (Text)
- **役割:**
  - 画像は見ない（画像由来のノイズや幻覚を遮断）。
  - JSONデータのみを元に「キュン方程式」等のルールを適用して発話する。
  - テンプレート選択方式ではなく、LLMによる動的生成（Dynamic Generation）を行うが、入力が固定化されているためブレない。

## 3. 実装仕様
- **推論エンジン:** LMDeploy (TurboMind backend)
- **モデル:** Qwen/Qwen2-VL-2B-Instruct (BF16 or Int8)
- **APIサーバー:** FastAPI (kotaro_api.py)
  - `/generate` エンドポイント内で、LMDeploy APIを2回呼び出す（Pass A -> Pass B）。

## 4. 期待される効果
- **安定性:** VRAM使用量を4GB〜5GB程度に抑え、PC動作を軽快に保つ。
- **再現性:** Pass B が画像ノイズの影響を受けないため、同じJSONからは常に一貫したキャラ性のコメントが生成される。
- **保守性:** 依存ライブラリを最新版 (`transformers >= 4.49`) で統一できるため、将来的なアップデートが容易。
