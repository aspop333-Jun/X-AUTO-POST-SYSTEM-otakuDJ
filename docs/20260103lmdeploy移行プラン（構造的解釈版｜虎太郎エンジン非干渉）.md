# LMDeploy移行プラン（構造的解釈版｜虎太郎エンジン非干渉）

【議論層：戦略／実務】

## 0. 前提（守るべき制約）
- **X投稿の虎太郎エンジン（スコアリング／パターン／テンプレ）は完成済み**：変更しない。
- 目的は **画像→文章（=構造的事実・特徴の抽出）** の精度と再現性を上げること。
- アプローチは **構造的解釈** のみ。感情・演出の強い路線（例：過度な煽り語彙）は採用しない。

---

## 1) プランの良いところ（そのまま採用でOK）
### ✅ Ollama → LMDeploy にする意味（設計上の勝ち）
- **LMDeployは TurboMind（C++/CUDA 推論エンジン）** を背後に持ち、GPU推論を高速・安定に回す土台になる。
- `lmdeploy serve api_server` により **OpenAI互換API** として立てられる（`/v1/chat/completions`）。
- **Qwen-VL-Chat-Int4 は Hugging Face 公式配布**で、VLMとして **画像→文章** の推論が可能。

> この時点で「画像→文章」の**認識エンジン**として成立する。

---

## 2) ここだけ直すと“勝ちが確定”する（重要）
### ❗「認識」と「表現」を統合しない
- 「テキスト生成もQwen-VL側で統合」は **採用しない**。
- 理由：虎太郎の価値は **既存のパターン×テンプレによる再現性**にある。
- 画像認識モデルに文章表現の全責任を背負わせると、**文体・テンプレ運用の安定性が崩れる**。

### 推奨構造（分離が正解）
- **LMDeploy + Qwen-VL-Chat-Int4：認識専用（画像→事実）**
- **虎太郎生成（既存）：表現専用（事実→投稿文）**

> 「目（認識）」と「口（表現）」を分けることで、**再現性が最大化**する。

---

## 3) OPUSプランを“完成形”に修正した実装計画（最小・強い）
### 変更概要（修正版）
| 項目 | 変更前 | 変更後（推奨） |
|---|---|---|
| 認識（画像→文章） | Ollama/混在 | **LMDeploy + Qwen-VL-Chat-Int4** |
| 虎太郎生成（文章） | 既存 | **既存のまま維持**（後で最適化可） |
| API | Ollama系 | **OpenAI互換 `/v1/chat/completions`** |

---

## 4) 「君のシステムがどう変わるか」を1枚で
### いま（混線しやすい）
```
画像 →（Ollama/モデル混在）→ 認識も生成も一気にやる → ブレる
```

### これから（役割が固定される）
```
画像
 ↓
[LMDeploy + Qwen-VL-Chat-Int4]  ← 画像を“事実”に変換（安定）
 ↓（JSON化推奨）
[虎太郎生成（既存）]           ← 事実を“テンプレ運用”で投稿化（再現）
```

---

## 5) 実務：このプランで作るべきファイル（最小セット）
### ✅ 新規
- `setup_lmdeploy.sh`
  - WSL2で venv作成 → LMDeploy導入 → 依存導入
- `start_lmdeploy.sh`
  - **nvidia-smi でGPU存在チェック（なければ即終了）**
  - LMDeploy APIサーバ起動（ポート `23333`）

### ✅ 修正
- `kotaro_api.py`
  - **Ollama呼び出しを廃止**
  - `http://localhost:23333/v1/chat/completions` へPOST（OpenAI互換）

### ✅ 保持（おすすめ）
- `vision_core.py`
  - 削除せず、**フォールバック用に保管**
  - 予期せぬVLM不調時の逃げ道（運用保険）

---

## 6) 重要：APIリクエスト形式（画像入力）
- LMDeployのOpenAI互換サーバーは、`messages[].content` に **text + image_url** を混ぜる形式を採用。
- 統合先はこの形式を守れば良い。

> 推奨：VLMの出力は「説明文」ではなく、**60基準や同点判定の“形式固定”**で返させる（再現性重視）。

---

## 7) 判断座標（設計の狙い）
- **構造美（可士和）**：認識と生成を分離 → 入出力が固定 → 再現性が生まれる
- **戦略理（孔明）**：Ollamaは初速の道具、LMDeployは勝ち切る道具
- **意思決定（Junmai）**：この移行は速度ではなく、**品質と再現性の投資**。虎太郎計画に直結

---

## 次にやること（即日で前に進める）
1. `setup_lmdeploy.sh` / `start_lmdeploy.sh` を作る（GPU強制起動）
2. `kotaro_api.py` を「OpenAI互換HTTPクライアント」に差し替える（Ollama廃止）
3. VLM出力を **形式固定**（60基準0/1、同点は番号のみ）に寄せて、スコアリングに接続

---

## 補足：虎太郎エンジンへ“影響を出さない”ための原則
- **変更箇所はVLM呼び出し部分のみ**（画像→構造判定の取得）
- KotaroScorerの
  - `PATTERNS`
  - `CRITERIA`
  - `SUB_ORDER`
  - tie-breakロジック
  - テンプレ運用
 には触れない。

> つまり「精度が上がるのは入力（判定）のみ」。出力の運用は固定される。

