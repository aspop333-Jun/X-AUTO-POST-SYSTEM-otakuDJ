# **消費者向けLLMラッパーの限界を超えて：Ubuntu (WSL2) 上でのエンタープライズグレード推論エンジン（vLLM/llama.cpp）による完全GPU駆動型インフラストラクチャの構築**

## **1\. エグゼクティブサマリーとアーキテクチャの転換**

現在のローカルLLM（Large Language Model）環境において、多くのユーザーはOllamaやそれに付随するGUIフロントエンド（ユーザーが言及する「虎太郎（Kotaro）」システムの一部と推測されるインターフェース層）を通じて生成AIを利用しています。これらは導入の障壁を下げる一方で、推論性能とリソース管理の観点から明確な「限界」を抱えています。ユーザーが直面している「システムが限界ではないか」という疑念は技術的に正当なものであり、特にOllamaが採用している「透過的なリソース管理（CPUへの自動フォールバック）」は、高性能な推論を求めるユーザーにとっては予測不可能性という致命的な欠陥となり得ます。

本レポートは、Ollamaのようなブラックボックス化されたラッパーから脱却し、**Ubuntu (WSL2)** 環境下で **vLLM** および **llama.cpp server** というエンタープライズグレードの推論エンジンを直接運用するための包括的な技術戦略書です。ユーザーの最優先事項である「必ずGPUで起動する（CPU処理による遅延を許容しない）」という要件を満たすため、リソース割り当てを厳格に管理し、ハードウェアの限界性能を引き出すための構成を詳述します。

推奨されるアーキテクチャは、NVIDIA GPU上でのスループットを最大化する **vLLM** を主軸とし、リソース制約が極めて厳しい場合に備えた **llama.cpp（サーバーモード）** を補助的に配置する構成です。また、対象モデルとして挙げられた **Qwen2.5**（推論・コーディング能力に優れる）および **MiniCPM-V**（視覚・マルチモーダル能力に優れる）の展開方法についても、VRAM容量に応じた量子化戦略（AWQ/GPTQ）を含めて詳細に分析します。

## ---

**2\. 現行システムの構造的限界：なぜ「オラマ」では不十分なのか**

ユーザーが感じている「限界」の本質を理解するためには、Ollamaおよびコンシューマー向けラッパーが抱えるアーキテクチャ上のトレードオフを解析する必要があります。これらは「使いやすさ」を最優先に設計されており、「決定論的な性能」は犠牲にされています 1。

### **2.1 不透明なCPUフォールバックとPCIeボトルネック**

Ollamaの最大の特徴であり、かつハイエンドユーザーにとっての最大の欠点は、メモリ管理の自動化です。モデルサイズがGPUのVRAM容量を超過しそうになった際、Ollamaはエラーを出して停止するのではなく、モデルの一部（レイヤー）をシステムRAM（メインメモリ）に退避させ、CPUで計算を続行します。GPUのメモリ帯域（HBM2/GDDR6Xなど）が秒間数百GB〜1TBを超えるのに対し、CPUとGPUを結ぶPCIeバスの帯域はせいぜい数十GB/sに過ぎません 3。  
この自動フォールバックが発生すると、生成速度は劇的に低下します（例：50トークン/秒から3トークン/秒へ）。ユーザーが求める「必ずGPU起動ができる」システムとは、リソース不足時には起動そのものを拒否し、稼働するならば100%のGPU性能を保証するシステムです。これを実現するには、メモリ管理の権限をユーザー側に取り戻す必要があります。

### **2.2 量子化フォーマットの非効率性**

Ollamaは主にGGUFフォーマットを使用します。GGUFはApple SiliconやCPU/GPUのハイブリッド推論には最適ですが、NVIDIA GPU単体での推論においては、**AWQ (Activation-aware Weight Quantization)** や **GPTQ** といったGPU専用のフォーマットと比較してカーネルレベルでの最適化が劣る場合があります 4。Qwen2.5のような高性能モデルを扱う際、GGUFに固執することはハードウェアリソースの浪費につながります。

### **2.3 並列処理とバッチングの欠如**

Ollamaは基本的にシングルユーザー、シングルストリームの処理を想定しています。これに対し、本レポートで推奨するvLLMは「Continuous Batching（連続バッチング）」技術を採用しており、複数のリクエストや長いコンテキスト（文脈）を効率的に処理できます。ユーザーがQwen2.5での高度な推論やMiniCPM-Vでの画像処理を行う場合、このバックエンドの違いは応答速度（レイテンシ）と処理量（スループット）に決定的な差をもたらします 6。

## ---

**3\. Ubuntu (WSL2) インフラストラクチャの構築と最適化**

ユーザーの要望通り、Windows上のOllamaから脱却し、Ubuntu環境へ移行します。現在、最も効率的かつ現実的な選択肢は **WSL2 (Windows Subsystem for Linux 2\)** です。WSL2は、GPUの準仮想化（GPU-PV）を通じて、Windowsホスト側のNVIDIAドライバーをLinuxカーネルから直接利用することを可能にします 8。

### **3.1 厳格なGPU認識プロセスの確立**

「必ずGPU起動ができる」という要件を満たすため、推論エンジンを起動する前に、システムレベルでGPUの可用性を検証するメカニズムを導入します。これはアプリケーション任せにせず、シェルスクリプトによって物理的なGPUステータスを確認し、条件を満たさない場合はプロセスを即座に遮断（Fail-Fast）する設計です。

以下に示すスクリプトは、nvidia-smi コマンドを用いてGPUの認識数と空きVRAM容量を直接問い合わせます。これをエンジンの起動スクリプトに組み込むことで、不確実性を排除します 10。

**表 3-1: GPU検証用Bashスクリプト (verify\_gpu.sh) の仕様**

| 行番 | 処理内容 | 目的 |
| :---- | :---- | :---- |
| 1-5 | nvidia-smi コマンドの存在確認 | ドライバーおよびWSL2パススルーが正常に機能しているかの基本チェック。 |
| 6-10 | GPU検出数の確認 | 認識されているGPUが0台の場合、即座にエラー終了させる。 |
| 11-15 | 空きVRAM容量の取得 | モデル展開に必要な最低限のVRAM（例：Qwen2.5-14Bなら約10GB）が空いているか数値で確認。 |
| 16-20 | 判定ロジック | 条件未達の場合、警告ではなくエラーコード 1 を返して後続のエンジン起動を阻止する。 |

このスクリプト運用により、ユーザーは「起動したのに遅い（実はCPUで動いている）」という事態を完全に回避できます。これは、システム管理の観点から非常に重要です。

### **3.2 WSL2 メモリ空間の最適化 (.wslconfig)**

WSL2はデフォルトでホストメモリの50%を使用しますが、大規模なLLM（特にQwen2.5-32Bクラス）を展開する場合、モデルのロード時に一時的に大量のRAMを消費し、Linux側のOOM Killer（メモリ不足時のプロセス強制終了機能）が発動するリスクがあります。  
これを防ぐため、Windows側のユーザープロファイル直下に .wslconfig ファイルを作成し、Ubuntuへ割り当てるメモリ量を明示的に最大化する必要があります。例えば、ホストが32GBのRAMを持つ場合、24GB〜26GB程度をWSL2に割り当てることで、モデルロード時の安定性を確保します。また、localhostForwarding=true を設定することで、Ubuntu内で立ち上げたAPIサーバー（vLLMなど）に対して、Windows側のクライアント（虎太郎などのUIツール）から localhost:8000 で透過的にアクセス可能になります 13。

## ---

**4\. 推奨エンジン：vLLMによるQwen2.5とMiniCPM-Vの展開**

本レポートが最も推奨する解決策は、カリフォルニア大学バークレー校で開発された **vLLM** の採用です。vLLMは、Ollamaのようなコンシューマー向けラッパーとは一線を画し、データセンターでの高負荷運用を想定して設計されています。その最大の特徴は「PagedAttention」アルゴリズムによるメモリ管理の効率化と、GPUリソースの占有制御にあります 6。

### **4.1 vLLMが「完全GPU駆動」に最適である理由**

vLLMは起動時に、指定されたGPUメモリの大部分（デフォルトで90%）をKVキャッシュ（文脈記憶領域）用として一括で確保します。この挙動は、以下の理由からユーザーの要件に合致します。

1. **メモリの事前確保:** プロセス起動時にVRAMをロックするため、実行中にメモリ不足でCPUへスワップアウトすることが構造上ありません。VRAMが足りない場合は起動時に ValueError を吐いて即座に停止します。これがユーザーの求める「GPUで動くか、動かないか」の二択（決定論的挙動）を保証します 14。  
2. **高速な推論カーネル:** NVIDIA GPUに最適化されたCUDAカーネルを使用しており、特にQwen2.5のような最新モデルアーキテクチャに対して、Ollama (llama.cpp) よりも高いスループットを発揮します。

### **4.2 Qwen2.5 (QUEN) の展開戦略**

Qwen2.5は、コーディングと論理推論において卓越した性能を持つモデルです。ユーザーは「QUENがいいが、Mini（小型モデル）でも仕方ない」と述べており、これは使用しているGPUのVRAM容量（おそらく12GB〜16GBクラスのコンシューマーGPU）と、モデルサイズ（32Bモデルなど）のトレードオフを懸念していると推測されます。

#### **4.2.1 モデルサイズの選定とVRAM要件**

Qwen2.5をGPUに完全に載せるためのVRAM要件は以下の通りです。ユーザーのハードウェア環境に応じた選択が必要です 4。

**表 4-1: Qwen2.5 モデルサイズ別 VRAM所要量概算（コンテキスト長 8192トークン時）**

| モデル | 量子化方式 | 推定VRAM使用量 | 適合GPU例 | 備考 |
| :---- | :---- | :---- | :---- | :---- |
| **Qwen2.5-32B** | **AWQ (4-bit)** | **約 18.5 GB** | RTX 3090/4090 (24GB) | **推奨:** VRAMが24GBあればこれがベスト。 |
| Qwen2.5-32B | FP16 | 約 65 GB | A100 x2 / Mac Studio | コンシューマーGPU単体では不可。 |
| **Qwen2.5-14B** | **AWQ (4-bit)** | **約 9.5 GB** | RTX 3060/4070 (12GB) | **推奨:** 12GB/16GB VRAM環境での最適解。 |
| Qwen2.5-14B | FP16 | 約 29 GB | RTX 3090/4090 (24GB) | 16GB VRAMでは不可。 |
| Qwen2.5-7B | FP16 | 約 15 GB | RTX 4060 Ti (16GB) | 7BであればFP16でも動作可能。 |

ユーザーの環境がもしRTX 3060や4070などの12GB VRAMクラスであれば、無理に32BモデルをCPUオフロード併用で動かすよりも、**Qwen2.5-14B-Instruct-AWQ** を選択し、vLLM上で完全GPU駆動させる方が、応答速度と体験の質は圧倒的に向上します。これが「Miniでも仕方ない」という懸念に対する、技術的に正しい回答となります。

#### **4.2.2 起動コマンドの構成**

vLLMを用いてQwen2.5を起動する際は、以下のフラグを用いてGPU利用を強制します。

Bash

python3 \-m vllm.entrypoints.openai.api\_server \\  
  \--model Qwen/Qwen2.5-14B-Instruct-AWQ \\  
  \--quantization awq \\  
  \--dtype half \\  
  \--gpu-memory-utilization 0.95 \\  
  \--max-model-len 8192 \\  
  \--tensor-parallel-size 1 \\  
  \--trust-remote-code \\  
  \--enforce-eager

* \--quantization awq: AWQフォーマットを使用し、GPUカーネルでの高速処理を指定。  
* \--gpu-memory-utilization 0.95: VRAMの95%をvLLMが占有することを許可します。これにより他のプロセスによるVRAM圧迫を防ぎます 13。  
* \--max-model-len 8192: コンテキスト長を制限することで、KVキャッシュに必要なメモリ量を抑制し、モデル本体のための領域を確保します。  
* \--enforce-eager: WSL2環境特有のCUDA Graphキャプチャ時のメモリエラーを回避し、安定性を向上させます 17。

### **4.3 MiniCPM-V 2.6 (マルチモーダル) の展開**

ユーザーが言及する「Mini」が、Qwenの小型版ではなく、**MiniCPM-V**（OpenBMB開発の高性能マルチモーダルモデル）を指している可能性も考慮すべきです。MiniCPM-V 2.6は8Bパラメータクラスでありながら、GPT-4Vに匹敵する視覚認識能力を持ちます。以前はvLLMでのサポートが限定的でしたが、現在はサポートが強化されています 18。

#### **4.3.1 マルチモーダル特有の構成**

MiniCPM-V 2.6をvLLMで動かす場合、画像エンコーダー（SigLip）とLLMデコーダー（Qwen2ベース）の両方がGPU上で動作する必要があります。ここでのポイントは、画像処理分のメモリオーバーヘッドを考慮することです。

Bash

python3 \-m vllm.entrypoints.openai.api\_server \\  
  \--model openbmb/MiniCPM-V-2\_6 \\  
  \--trust-remote-code \\  
  \--max-model-len 4096 \\  
  \--limit-mm-per-prompt image=1 \\  
  \--dtype auto

* \--limit-mm-per-prompt: 1プロンプトあたりの画像枚数を制限し、予期せぬメモリ溢れを防ぎます 20。  
* \--trust-remote-code: MiniCPM-Vはカスタムモデル定義を使用するため、このフラグが必須です。これを忘れるとロード自体が失敗します。

## ---

**5\. 代替案としてのllama.cpp Server：ベアメタル制御**

もしvLLMのメモリ予約機能（VRAMの90%を確保する挙動）が、ユーザーの他の作業（ブラウザやゲームなど）を阻害する場合、あるいはAWQモデルではなく手持ちのGGUFモデル資産を活用したい場合は、**llama.cpp server** の直接運用が次善の策となります。ただし、Ollama経由ではなくバイナリを直接叩くことで、CPUフォールバックを無効化する運用を行います。

### **5.1 コンパイルによるGPUバックエンドの固定**

Ubuntu上で llama.cpp をソースコードからビルドする際、CUDAサポートのみを有効にし、CPU計算用のライブラリ依存を極小化することが可能です。

Bash

git clone https://github.com/ggml-org/llama.cpp  
cd llama.cpp  
make clean  
GGML\_CUDA=1 make \-j$(nproc) llama-server

GGML\_CUDA=1 を指定することで、CUDAバックエンドを強力に優先するバイナリが生成されます 21。

### **5.2 「Fail-Fast」を実現する起動パラメータ**

llama.cppには「CPUフォールバックを禁止する」という直接的なフラグは存在しませんが、オフロード層数をモデルの総層数以上に設定することで、事実上のGPU強制が可能です。

**起動コマンド例:**

Bash

./llama-server \\  
  \-m /models/Qwen2.5-32B-Instruct-Q4\_K\_M.gguf \\  
  \--n-gpu-layers 999 \\  
  \--main-gpu 0 \\  
  \--ctx-size 8192 \\  
  \--port 8080 \\  
  \--host 0.0.0.0

* \--n-gpu-layers 999 (または \-ngl 999): モデルのレイヤー数（例えば80層）よりも遥かに大きい数値を指定することで、全てのレイヤーをGPUに転送しようとします。VRAMが不足していれば、Ollamaのように勝手にCPUを使うのではなく、メモリエラーでクラッシュするか、あるいはログに警告が出ます 23。

### **5.3 ログ監視による完全性の担保**

llama.cppの場合、VRAM不足時に「一部をCPUで実行しました」というログを出して動き続ける場合があります。これを許容しないために、以下のような監視ラッパーを介して起動します。

Bash

\# ログを監視し、"CPU"という文字列が計算デバイスとして割り当てられたらプロセスを殺す  
./llama-server... \> server.log 2\>&1 &  
PID=$\!  
sleep 5  
if grep \-q "assigned to device CPU" server.log; then  
    echo "エラー: GPU VRAM不足のためCPUへのオフロードが発生しました。プロセスを停止します。"  
    kill $PID  
    exit 1  
fi

これにより、ユーザーが恐れる「気づかないうちにCPUで動いている」状況をプログラム的に排除できます。

## ---

**6\. クライアント（虎太郎/Kotaro）との接続統合**

バックエンドをOllamaからvLLMやllama.cpp serverに変更した場合、フロントエンド（虎太郎システム）の設定変更が必要です。幸いなことに、vLLMもllama.cpp serverも **OpenAI互換 API** を提供しています。

### **6.1 APIエンドポイントの変更**

Ollamaは通常 http://localhost:11434 で動作しますが、vLLMやllama.cppはデフォルトで http://localhost:8000 または 8080 を使用します。  
ユーザーは虎太郎側の設定で以下のように変更を行う必要があります。

* **API Type:** OpenAI (Compatible)  
* **Base URL:** http://localhost:8000/v1 (vLLMの場合)  
* **API Key:** EMPTY (vLLMはデフォルトでキー認証なし、または任意の文字列で可)  
* **Model Name:** 起動時に指定したモデル名（例: Qwen/Qwen2.5-14B-Instruct-AWQ）

### **6.2 画像入力（MiniCPM-V）の互換性**

MiniCPM-V 2.6をvLLM経由で使用する場合、画像データはOpenAI Vision API形式（image\_url パラメータ）で送信する必要があります。虎太郎システムが画像をBase64エンコードしてこの形式で送信できるかどうかが鍵となりますが、近年の多くのLLMフロントエンドはOpenAI Vision形式に対応しているため、バックエンドをvLLMに切り替えるだけで、高速な画像認識が可能になる公算が高いです 25。

## ---

**7\. 結論と推奨ロードマップ**

ユーザーの懸念である「オラマシステムの限界」は、ソフトウェアによる安全策（CPUフォールバック）がボトルネックとなっている事実を正確に捉えています。これを突破し、Ubuntu環境でQwenやMiniCPMの性能を解き放つためのロードマップは以下の通りです。

1. **インフラ:** Windows上でNVIDIAドライバーを整備し、WSL2 Ubuntu環境を構築する。.wslconfig でメモリ制限を緩和する。  
2. **エンジン:** **vLLM** を第一選択とする。これにより、「メモリ不足時は起動しない（Fail-Fast）」という厳格なGPU運用が実現され、Qwen2.5やMiniCPM-Vの推論速度が最大化される。  
3. **モデル選定:** VRAM容量に基づき、**Qwen2.5-14B-AWQ** (12-16GB VRAM向け) または **Qwen2.5-32B-AWQ** (24GB VRAM向け) を選択する。「Miniでも仕方ない」という妥協は、14Bモデルの採用によって、品質を大きく落とさずに解決可能である。  
4. **運用:** 独自の起動スクリプトを用いてGPUの状態を常に監視し、不確実性を排除する。

この構成への移行により、システムは「なんとなく動く」状態から「設計通りに高性能に動く」状態へと進化し、ユーザーの期待する応答速度と安定性を実現できるでしょう。

#### **引用文献**

1. 5 Awesome Ollama Alternatives You Should Know \- Sliplane, 1月 3, 2026にアクセス、 [https://sliplane.io/blog/5-awesome-ollama-alternatives](https://sliplane.io/blog/5-awesome-ollama-alternatives)  
2. The Ultimate LLM Inference Battle: vLLM vs. Ollama vs. ZML, 1月 3, 2026にアクセス、 [https://dev.to/worldlinetech/the-ultimate-llm-inference-battle-vllm-vs-ollama-vs-zml-m97](https://dev.to/worldlinetech/the-ultimate-llm-inference-battle-vllm-vs-ollama-vs-zml-m97)  
3. Stop Wasting Your Multi-GPU Setup With llama.cpp, 1月 3, 2026にアクセス、 [https://www.ahmadosman.com/blog/do-not-use-llama-cpp-or-ollama-on-multi-gpus-setups-use-vllm-or-exllamav2/](https://www.ahmadosman.com/blog/do-not-use-llama-cpp-or-ollama-on-multi-gpus-setups-use-vllm-or-exllamav2/)  
4. GPU Inference VRAM Calc for Qwen2.5-Coder 32B \- Reddit, 1月 3, 2026にアクセス、 [https://www.reddit.com/r/LocalLLaMA/comments/1greuto/gpu\_inference\_vram\_calc\_for\_qwen25coder\_32b\_need/](https://www.reddit.com/r/LocalLLaMA/comments/1greuto/gpu_inference_vram_calc_for_qwen25coder_32b_need/)  
5. qwen2.5-coder:32b \- Ollama, 1月 3, 2026にアクセス、 [https://ollama.com/library/qwen2.5-coder:32b](https://ollama.com/library/qwen2.5-coder:32b)  
6. vLLM or llama.cpp: Choosing the right LLM inference engine for ..., 1月 3, 2026にアクセス、 [https://developers.redhat.com/articles/2025/09/30/vllm-or-llamacpp-choosing-right-llm-inference-engine-your-use-case](https://developers.redhat.com/articles/2025/09/30/vllm-or-llamacpp-choosing-right-llm-inference-engine-your-use-case)  
7. Performance vs Practicality: A Comparison of vLLM and Ollama, 1月 3, 2026にアクセス、 [https://robert-mcdermott.medium.com/performance-vs-practicality-a-comparison-of-vllm-and-ollama-104acad250fd](https://robert-mcdermott.medium.com/performance-vs-practicality-a-comparison-of-vllm-and-ollama-104acad250fd)  
8. How to use NVIDIA GPUs on a Windows notebook with Linux, 1月 3, 2026にアクセス、 [https://blog.nashcom.de/nashcomblog.nsf/dx/how-to-use-nvidia-gpus-on-a-windows-notebook-with-linux.htm?opendocument\&comments](https://blog.nashcom.de/nashcomblog.nsf/dx/how-to-use-nvidia-gpus-on-a-windows-notebook-with-linux.htm?opendocument&comments)  
9. CUDA, Ollama, Docker & Stable Diffusion Setup \- blackMORE Ops, 1月 3, 2026にアクセス、 [https://www.blackmoreops.com/wsl-ai-development-setup-guide/](https://www.blackmoreops.com/wsl-ai-development-setup-guide/)  
10. How to check if nvidia-gpu is available using bash script?, 1月 3, 2026にアクセス、 [https://stackoverflow.com/questions/66611439/how-to-check-if-nvidia-gpu-is-available-using-bash-script](https://stackoverflow.com/questions/66611439/how-to-check-if-nvidia-gpu-is-available-using-bash-script)  
11. How do I check if my NVIDIA GPU is detected by the Linux system?, 1月 3, 2026にアクセス、 [https://massedcompute.com/faq-answers/?question=How%20do%20I%20check%20if%20my%20NVIDIA%20GPU%20is%20detected%20by%20the%20Linux%20system?](https://massedcompute.com/faq-answers/?question=How+do+I+check+if+my+NVIDIA+GPU+is+detected+by+the+Linux+system?)  
12. Nvidia-smi Manual, 1月 3, 2026にアクセス、 [https://docs.nvidia.com/deploy/nvidia-smi/index.html](https://docs.nvidia.com/deploy/nvidia-smi/index.html)  
13. Install vLLM in WSL \- mobiarch, 1月 3, 2026にアクセス、 [https://mobiarch.wordpress.com/2025/10/02/install-vllm-in-wsl/](https://mobiarch.wordpress.com/2025/10/02/install-vllm-in-wsl/)  
14. vLLM server arguments | Red Hat AI Inference Server | 3.1, 1月 3, 2026にアクセス、 [https://docs.redhat.com/en/documentation/red\_hat\_ai\_inference\_server/3.1/html-single/vllm\_server\_arguments/index](https://docs.redhat.com/en/documentation/red_hat_ai_inference_server/3.1/html-single/vllm_server_arguments/index)  
15. Troubleshooting \- vLLM, 1月 3, 2026にアクセス、 [https://docs.vllm.ai/en/latest/usage/troubleshooting/](https://docs.vllm.ai/en/latest/usage/troubleshooting/)  
16. Qwen/Qwen2.5-32B-Instruct-GPTQ-Int4 \- Hugging Face, 1月 3, 2026にアクセス、 [https://huggingface.co/Qwen/Qwen2.5-32B-Instruct-GPTQ-Int4](https://huggingface.co/Qwen/Qwen2.5-32B-Instruct-GPTQ-Int4)  
17. vLLM \- Qwen, 1月 3, 2026にアクセス、 [https://qwen.readthedocs.io/en/v2.5/deployment/vllm.html](https://qwen.readthedocs.io/en/v2.5/deployment/vllm.html)  
18. openbmb/minicpm-v2.6 \- Ollama, 1月 3, 2026にアクセス、 [https://ollama.com/openbmb/minicpm-v2.6](https://ollama.com/openbmb/minicpm-v2.6)  
19. MiniCPM-V-2\_6-vllm model | Clarifai \- The World's AI, 1月 3, 2026にアクセス、 [https://clarifai.com/openbmb/miniCPM/models/MiniCPM-V-2\_6-vllm](https://clarifai.com/openbmb/miniCPM/models/MiniCPM-V-2_6-vllm)  
20. Qwen2.5-VL Usage Guide \- vLLM Recipes, 1月 3, 2026にアクセス、 [https://docs.vllm.ai/projects/recipes/en/latest/Qwen/Qwen2.5-VL.html](https://docs.vllm.ai/projects/recipes/en/latest/Qwen/Qwen2.5-VL.html)  
21. llama.cpp guide \- Running LLMs locally, on any hardware, from ..., 1月 3, 2026にアクセス、 [https://steelph0enix.github.io/posts/llama-cpp-guide/](https://steelph0enix.github.io/posts/llama-cpp-guide/)  
22. Llama.cpp guide – Running LLMs locally on any hardware, from ..., 1月 3, 2026にアクセス、 [https://news.ycombinator.com/item?id=42274489](https://news.ycombinator.com/item?id=42274489)  
23. How to run big MoE models like Qwen-3–235B-A22B in Llama.cpp ..., 1月 3, 2026にアクセス、 [https://medium.com/@david.sanftenberg/gpu-poor-how-to-configure-offloading-for-the-qwen-3-235b-a22b-moe-model-using-llama-cpp-13dc15287bed](https://medium.com/@david.sanftenberg/gpu-poor-how-to-configure-offloading-for-the-qwen-3-235b-a22b-moe-model-using-llama-cpp-13dc15287bed)  
24. ggml-org/llama.cpp \- non-CPU compilation and forcing GPU \- GitHub, 1月 3, 2026にアクセス、 [https://github.com/ggml-org/llama.cpp/issues/12346](https://github.com/ggml-org/llama.cpp/issues/12346)  
25. MiniCPM-V 2.6 Deployment Tutorial, 1月 3, 2026にアクセス、 [https://modelbest.feishu.cn/wiki/C2BWw4ZP0iCDy7kkCPCcX2BHnOf?from=from\_copylink](https://modelbest.feishu.cn/wiki/C2BWw4ZP0iCDy7kkCPCcX2BHnOf?from=from_copylink)  
26. \[BUG\] \<使用vLLM 运行MiniCPM-Llama3-V-2\_5， 启动后台服务后 ..., 1月 3, 2026にアクセス、 [https://github.com/OpenBMB/MiniCPM-V/issues/373](https://github.com/OpenBMB/MiniCPM-V/issues/373)