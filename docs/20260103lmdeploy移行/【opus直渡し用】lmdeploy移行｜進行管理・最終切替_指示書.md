# 【opus直渡し用】LMDeploy移行｜進行管理・最終切替 指示書

**対象**：opus（実装担当）  
**目的**：Ollama依存の現行VLM基盤を、**LMDeploy + Qwen‑VL‑Chat‑Int4** に移行する。  
**最重要条件**：事故ゼロ／ロールバック可能／最後に一気に切替。

---

## 0. 絶対条件（先に読む）

- **虎太郎エンジンには一切触るな**（KotaroScorer / PATTERNS / CRITERIA / SUB_ORDER）
- **削除禁止**：Ollama関連コード・`vision_core.py`
- **GPUが使えなければ即停止**（CPUフォールバック禁止）
- **途中で本番を切り替えるな**。切替は最後の1回だけ

これを破るなら作業しないこと。

---

## 1. 作業の考え方（順番がすべて）

やることは増えていない。**順番を変えるだけ**。

1. 先に「動く別系統」を裏で完成させる
2. 次に呼び出し口だけを差し替える
3. 最後に一気に切る

この順番を崩すと事故る。

---

## 2. フェーズ構成（厳守）

### Phase 1｜準備（本番に影響ゼロ）

**目的**：LMDeploy単体を“完成品”にする

やること：
- WSL2 + Docker + NVIDIA Runtime 構築
- LMDeploy コンテナ起動（ポート **23333 固定**）
- Qwen‑VL‑Chat‑Int4 で単画像推論

**合格条件**（全部YESになるまで次へ進むな）：
- GPU認識OK
- JSONが安定して返る
- 推論時間 < 2秒/画像

👉 この段階では **Ollamaに一切触るな**

---

### Phase 2｜接続（差し替え最小）

**目的**：呼び出し先だけを静かに変える

やること：
- `kotaro_api.py` の **VLM呼び出し部分のみ** 修正
- OpenAI互換 `/v1/chat/completions` を使用
- `response_format = json_object` を必須指定

**禁止事項**：
- スコア計算・判定ロジック変更
- プロンプト再設計
- JSONキー・基準数の変更

👉 「ついで作業」は全部禁止

---

### Phase 3｜最終切替（短時間・一発）

**実行条件（全てYES）**：
- JSON正常率 100%
- `trigger_match` 欠損なし
- `confidence` が 0〜1 で安定
- GPU未検出時に即Failすることを確認済み

**切替手順（この順で一気に）**：
1. Ollamaサービス停止
2. LMDeploy API を本番エンドポイントに固定
3. 再起動
4. 連続10件の正常動作確認
5. 完了宣言

途中で止めるな。迷うな。

---

## 3. トラブル時の即時対応

- 何かおかしいと感じたら **即ロールバック**
- 原因調査は後でいい
- ロールバックできない状態を作らない

---

## 4. 完了条件（Junmai確認）

- 認識精度が明確に改善している
- JSON揺らぎが消えている
- 速度が安定している
- 既存スコア結果に違和感がない

これを満たしたら作業終了。

---

## 5. 最後に

これは技術チャレンジじゃない。
**切替イベントの実行**だ。

準備は静かに、
切替は一瞬で。

以上。

