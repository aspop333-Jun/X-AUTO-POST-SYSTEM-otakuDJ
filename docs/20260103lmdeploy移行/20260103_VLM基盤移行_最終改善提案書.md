# 虎太郎エンジン VLM基盤移行 最終改善提案書

**作成日**: 2026-01-03  
**作成者**: Antigravity (ChatGPT/Gemini提案を統合)  
**状態**: 承認待ち

---

## Executive Summary

Ollama依存の現行VLM基盤を、**LMDeploy + Qwen-VL-Chat-Int4** に移行し、認識精度と再現性を最大化する。虎太郎エンジン（スコアリング/パターン/テンプレ）は**一切変更しない**。

---

## 1. 設計原則

| 原則 | 説明 |
|-----|------|
| **認識と表現の分離** | VLM=「目」、虎太郎=「口」。責任を分離し再現性を確保 |
| **非干渉** | KotaroScorer, PATTERNS, CRITERIA, SUB_ORDERは触らない |
| **Fail-Fast** | GPUなければ即終了。CPUフォールバックは禁止 |
| **形式固定** | VLM出力は60基準0/1のJSON。揺らぎは虎太郎側で管理 |

---

## 2. アーキテクチャ

```
┌─────────────────────────────────────────────────────────┐
│                    Windows (Host)                        │
│  ┌─────────────────────────────────────────────────┐    │
│  │               Next.js Frontend                   │    │
│  │            (localhost:3000)                      │    │
│  └──────────────────────┬──────────────────────────┘    │
│                         │ HTTP                          │
│  ┌──────────────────────▼──────────────────────────┐    │
│  │           Kotaro API (FastAPI)                   │    │
│  │            (localhost:8000)                      │    │
│  │  ┌─────────────────────────────────────────┐    │    │
│  │  │  KotaroScorer V2.3 (変更なし)            │    │    │
│  │  │  - PATTERNS (12)                        │    │    │
│  │  │  - CRITERIA (60)                        │    │    │
│  │  │  - SUB_ORDER (4連単)                    │    │    │
│  │  └─────────────────────────────────────────┘    │    │
│  └──────────────────────┬──────────────────────────┘    │
│                         │ HTTP (OpenAI互換)             │
│  ┌──────────────────────▼──────────────────────────┐    │
│  │              WSL2 Ubuntu-22.04                   │    │
│  │  ┌─────────────────────────────────────────┐    │    │
│  │  │  Docker Container (NVIDIA Runtime)      │    │    │
│  │  │  ┌───────────────────────────────────┐  │    │    │
│  │  │  │  LMDeploy API Server               │  │    │    │
│  │  │  │  + Qwen-VL-Chat-Int4               │  │    │    │
│  │  │  │  (localhost:23333)                 │  │    │    │
│  │  │  │  - JSON Schema強制出力             │  │    │    │
│  │  │  │  - KVキャッシュ量子化              │  │    │    │
│  │  │  └───────────────────────────────────┘  │    │    │
│  │  └─────────────────────────────────────────┘    │    │
│  │                    │                            │    │
│  │              RTX 4060 (8GB VRAM)                │    │
│  └─────────────────────────────────────────────────┘    │
└─────────────────────────────────────────────────────────┘
```

---

## 3. 採用する改善項目

### Phase 1: 即時採用 ✅

| # | 項目 | 出典 | 効果 |
|---|------|------|------|
| 1 | LMDeploy + Qwen-VL-Chat-Int4 | ChatGPT | 高速・安定GPU推論 |
| 2 | Dockerコンテナ化 | Gemini | 環境分離、復旧迅速化 |
| 3 | JSON Schema強制出力 | Gemini | パースエラー撲滅 |
| 4 | Fail-Fast GPU強制 | ChatGPT | CPUフォールバック禁止 |

### Phase 2: 中期導入

| # | 項目 | 出典 | 効果 |
|---|------|------|------|
| 5 | LiteLLM統合ゲートウェイ | Gemini | エンドポイント一元化 |
| 6 | KVキャッシュ量子化 | Gemini | VRAM節約 |

### Phase 3: 長期導入

| # | 項目 | 出典 | 効果 |
|---|------|------|------|
| 7 | Prometheus+Grafana監視 | Gemini | 可観測性強化 |

---

## 4. 実装ファイル一覧

### 新規作成

| ファイル | 説明 |
|---------|------|
| `docker/Dockerfile.lmdeploy` | LMDeploy + Qwen-VL用コンテナ |
| `docker/docker-compose.yml` | コンテナオーケストレーション |
| `scripts/setup_lmdeploy.sh` | WSL2環境構築スクリプト |
| `scripts/start_lmdeploy.sh` | GPU確認＋コンテナ起動 |

### 修正

| ファイル | 変更内容 |
|---------|---------|
| `kotaro_api.py` | Ollama呼び出し → OpenAI互換HTTP |
| `vision_core.py` | フォールバック用に保管（削除しない） |

### 変更なし（非干渉）

- `kotaro_scoring.py` (KotaroScorer V2.3)
- `pattern_mapping.py`
- `scoring_criteria_draft.md`

---

## 5. VLM出力形式（JSON Schema）

```json
{
  "criteria": {
    "A01": 1, "A02": 0, "A03": 1, ...  // 60基準の0/1
  },
  "trigger_match": "P02",  // トリガー判定結果
  "confidence": 0.85
}
```

```python
# Pydantic Schema
class VLMResponse(BaseModel):
    criteria: Dict[str, Literal[0, 1]]  # 60個
    trigger_match: Optional[str]
    confidence: float
```

---

## 6. 実装手順

### Step 1: Docker環境構築
```bash
# WSL2 Ubuntu
sudo apt update && sudo apt install -y docker.io
sudo systemctl enable docker
sudo usermod -aG docker $USER

# NVIDIA Container Toolkit
distribution=$(. /etc/os-release;echo $ID$VERSION_ID)
curl -s -L https://nvidia.github.io/nvidia-docker/gpgkey | sudo apt-key add -
curl -s -L https://nvidia.github.io/nvidia-docker/$distribution/nvidia-docker.list | \
  sudo tee /etc/apt/sources.list.d/nvidia-docker.list
sudo apt update && sudo apt install -y nvidia-container-toolkit
sudo systemctl restart docker
```

### Step 2: LMDeployコンテナ起動
```bash
docker run -d --gpus all \
  -p 23333:23333 \
  --name lmdeploy-vlm \
  openmmlab/lmdeploy:latest \
  lmdeploy serve api_server \
    Qwen/Qwen-VL-Chat-Int4 \
    --server-port 23333 \
    --cache-max-entry-count 0.4
```

### Step 3: kotaro_api.py修正
```python
# Before (Ollama)
response = ollama.generate(model="qwen2.5-vl", ...)

# After (OpenAI互換)
import httpx
response = httpx.post(
    "http://localhost:23333/v1/chat/completions",
    json={
        "model": "Qwen/Qwen-VL-Chat-Int4",
        "messages": [...],
        "response_format": {"type": "json_object"}
    }
)
```

---

## 7. リスク対策

| リスク | 対策 |
|-------|------|
| LMDeployダウン | vision_core.py をフォールバックとして保管 |
| VRAM不足 | KVキャッシュ量子化 + cache-max-entry-count調整 |
| JSON破損 | JSON Schema強制 + 3回リトライ |
| GPU未検出 | Fail-Fast（即終了、CPUフォールバック禁止） |

---

## 8. 成功指標

| 指標 | 目標 |
|-----|------|
| VLM推論時間 | < 2秒/画像 |
| JSON正常率 | 100% |
| パターン判定精度 | > 95% |
| システム稼働率 | > 99% |

---

## 承認

- [ ] 設計承認 (Junmai)
- [ ] Phase 1 実装開始

---

**次のアクション**: 承認後、Docker環境構築から開始
