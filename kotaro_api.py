"""
Kotaro-Engine API Server (V2.3 LMDeploy Edition)
"""
from fastapi import FastAPI, File, UploadFile, Form, HTTPException
from fastapi.middleware.cors import CORSMiddleware
import httpx
import uvicorn
import base64
import tempfile
import os
import json
import logging
from typing import List, Dict, Any, Optional
from kotaro_scoring import KotaroScorer, CRITERIA

# ãƒ­ã‚¬ãƒ¼è¨­å®š
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger("kotaro_api")

app = FastAPI(title="Kotaro-Engine API (V2.3)")

# CORS
app.add_middleware(
    CORSMiddleware,
    allow_origins=["*"], # é–‹ç™ºç”¨: Next.jsç­‰ã‹ã‚‰ã®ã‚¢ã‚¯ã‚»ã‚¹ã‚’è¨±å¯
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)

# ã‚¹ã‚³ã‚¢ãƒ©ãƒ¼åˆæœŸåŒ–
scorer = KotaroScorer()

# VLMè¨­å®š
# Docker(host network) or Port Forwarding
LMDEPLOY_URL = "http://localhost:23333/v1/chat/completions"

async def call_vlm_analysis(image_path: str) -> Dict[str, Any]:
    """VLMã«ç”»åƒã‚’æŠ•ã’ã¦60é …ç›®åˆ¤å®š(JSON)ã‚’å–å¾—"""
    
    # ç”»åƒã‚¨ãƒ³ã‚³ãƒ¼ãƒ‰
    with open(image_path, "rb") as f:
        b64_img = base64.b64encode(f.read()).decode("utf-8")
        
    # ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆæ§‹ç¯‰ï¼ˆJSON Schemaå¯¾å¿œï¼‰
    questions = [f"{c['id']}: {c['question']}" for c in CRITERIA]
    criteria_list = "\n".join(questions)
    
    system_prompt = """ã‚ãªãŸã¯ç”»åƒèªè­˜AIã§ã™ã€‚ä»¥ä¸‹ã®åˆ¤å®šåŸºæº–ã«åŸºã¥ãã€ç”»åƒã®å†…å®¹ã‚’åˆ†æã—ã¦ãã ã•ã„ã€‚
å‡ºåŠ›ã¯å¿…ãšJSONå½¢å¼ã§è¡Œã£ã¦ãã ã•ã„ã€‚ä½™è¨ˆãªæ–‡ç« ã¯ä¸€åˆ‡å«ã‚ãªã„ã§ãã ã•ã„ã€‚"""

    user_prompt = f"""ã“ã®å†™çœŸã‚’ä»¥ä¸‹ã®60é …ç›®ã§åˆ¤å®šã—ã€çµæœã‚’JSONã§å‡ºåŠ›ã—ã¦ãã ã•ã„ã€‚
è©²å½“ã™ã‚‹å ´åˆã¯1ã€ã—ãªã„å ´åˆã¯0ã§ã™ã€‚
ç¢ºä¿¡ãŒæŒã¦ãªã„å ´åˆã§ã‚‚ã€ã©ã¡ã‚‰ã‹ã‚’é¸ã‚“ã§ãã ã•ã„ã€‚

--- åˆ¤å®šé …ç›® ---
{criteria_list}

--- å‡ºåŠ›å½¢å¼ ---
{{
  "criteria": {{
    "A01": 1,
    "A02": 0,
    ...
  }},
  "confidence": 0.95
}}
"""

    payload = {
        "model": "Qwen/Qwen-VL-Chat-Int4",
        "messages": [
            {"role": "system", "content": system_prompt},
            {
                "role": "user", 
                "content": [
                    {"type": "text", "text": user_prompt},
                    {"type": "image_url", "image_url": {"url": f"data:image/jpeg;base64,{b64_img}"}}
                ]
            }
        ],
        "temperature": 0.1,  # æ±ºå®šè«–çš„ã«
        "max_tokens": 1024,
        # "response_format": {"type": "json_object"} # Qwen-VLã®å®Ÿè£…ä¾å­˜ã®ãŸã‚ã€ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆæŒ‡ç¤ºã‚’ãƒ¡ã‚¤ãƒ³ã«ã™ã‚‹
    }
    
    async with httpx.AsyncClient(timeout=60.0) as client:
        try:
            resp = await client.post(LMDEPLOY_URL, json=payload)
            resp.raise_for_status()
            result = resp.json()
            
            # ãƒ¬ã‚¹ãƒãƒ³ã‚¹æ§‹é€ ã®ç¢ºèª
            if "choices" not in result or len(result["choices"]) == 0:
                raise ValueError("Invalid VLM response format")
                
            content = result["choices"][0]["message"]["content"]
            logger.info(f"VLM Raw Response: {content[:100]}...") # ãƒ­ã‚°å‡ºåŠ›
            
            # JSONãƒ‘ãƒ¼ã‚¹ï¼ˆMarkdownã® ```json ... ``` ã‚’é™¤å»ï¼‰
            clean_content = content.replace("```json", "").replace("```", "").strip()
            return json.loads(clean_content)
            
        except json.JSONDecodeError:
            logger.error(f"JSON Parse Error. Content: {content}")
            raise HTTPException(status_code=500, detail="VLM returned invalid JSON")
        except Exception as e:
            logger.error(f"VLM Error: {e}")
            raise HTTPException(status_code=500, detail=f"VLM Analysis Failed: {str(e)}")

@app.post("/generate")
async def generate_comment(
    image: UploadFile = File(...),
    name: str = Form(default=""),
    count: int = Form(default=3),
):
    """ãƒ¡ã‚¤ãƒ³ã‚¨ãƒ³ãƒ‰ãƒã‚¤ãƒ³ãƒˆï¼šç”»åƒ -> VLM -> Scorer -> Comment"""
    
    # ç”»åƒä¸€æ™‚ä¿å­˜
    with tempfile.NamedTemporaryFile(delete=False, suffix=".jpg") as tmp:
        content = await image.read()
        tmp.write(content)
        tmp_path = tmp.name
        
    try:
        # 1. VLMåˆ†æï¼ˆ60é …ç›®åˆ¤å®šï¼‰
        logger.info("Calling VLM...")
        vlm_result = await call_vlm_analysis(tmp_path)
        criteria_answers = vlm_result.get("criteria", {})
        
        # 0/1 ã‚’ bool ã«å¤‰æ›
        answers_bool = {k: bool(v) for k, v in criteria_answers.items()}
        
        # 2. ã‚¹ã‚³ã‚¢ãƒªãƒ³ã‚°ï¼†ãƒ‘ã‚¿ãƒ¼ãƒ³æ±ºå®š
        logger.info("Scoring...")
        pattern_id, p_scores, s_scores = scorer.score_from_answers(answers_bool)
        pattern_info = scorer.patterns[pattern_id]
        
        # 3. ã‚³ãƒ¡ãƒ³ãƒˆç”Ÿæˆ
        comments = []
        for _ in range(count):
            raw_comment = scorer.get_comment(pattern_id)
            # åå‰å…¥ã‚Œ
            if name.strip() and not raw_comment.startswith(name):
                final_comment = f"{name}ã•ã‚“ã€{raw_comment}"
            else:
                final_comment = raw_comment
            comments.append(final_comment)
            
        # å¾Œæ–¹äº’æ›æ€§ãƒ•ã‚£ãƒ¼ãƒ«ãƒ‰
        expression_str = f"{pattern_info['name']} ({pattern_info['trigger']})"
            
        return {
            "success": True,
            "pattern": {
                "id": pattern_id,
                "name": pattern_info["name"],
                "trigger": pattern_info["trigger"]
            },
            "expression": expression_str, # ãƒ•ãƒ­ãƒ³ãƒˆã‚¨ãƒ³ãƒ‰äº’æ›æ€§
            "analysis": expression_str,   # æ—§APIäº’æ›æ€§
            "scores": p_scores,
            "comments": comments,
            "analysis_raw": vlm_result # ãƒ•ãƒ­ãƒ³ãƒˆã‚¨ãƒ³ãƒ‰ã§ã®ãƒ‡ãƒãƒƒã‚°ç”¨ã«ç¶­æŒ
        }
        
    except httpx.ConnectError:
        logger.error("VLM Connection Failed")
        return {
            "success": False,
            "error": "VLM engine is offline. Please start LMDeploy container."
        }
    except Exception as e:
        logger.error(f"API Error: {e}")
        return {"success": False, "error": str(e)}
        
    finally:
        if os.path.exists(tmp_path):
            os.unlink(tmp_path)

if __name__ == "__main__":
    print("\nğŸ¯ Kotaro-Engine API Server (V2.3)")
    print(f"Connecting to VLM: {LMDEPLOY_URL}")
    print("=" * 40)
    uvicorn.run(app, host="0.0.0.0", port=8000)
