"""
Kotaro-Engine API Server (V2.3 LMDeploy Edition)
"""
from fastapi import FastAPI, File, UploadFile, Form, HTTPException
from fastapi.middleware.cors import CORSMiddleware
import httpx
import uvicorn
import base64
import tempfile
import os
import json
import logging
import random
from typing import List, Dict, Any, Optional
from kotaro_scoring import KotaroScorer, CRITERIA
import google.generativeai as genai
from dotenv import load_dotenv

# Áí∞Â¢ÉÂ§âÊï∞„ÇíË™≠„ÅøËæº„ÇÄ
load_dotenv()

# „É≠„Ç¨„ÉºË®≠ÂÆö
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger("kotaro_api")

app = FastAPI(title="Kotaro-Engine API (V2.3 - Hybrid)")

# Gemini APIË®≠ÂÆö
GEMINI_API_KEY = os.getenv("GEMINI_API_KEY")
if GEMINI_API_KEY:
    genai.configure(api_key=GEMINI_API_KEY)

# CORS
app.add_middleware(
    CORSMiddleware,
    allow_origins=["*"], # ÈñãÁô∫Áî®: Next.jsÁ≠â„Åã„Çâ„ÅÆ„Ç¢„ÇØ„Çª„Çπ„ÇíË®±ÂèØ
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)

# „Çπ„Ç≥„Ç¢„É©„ÉºÂàùÊúüÂåñ
scorer = KotaroScorer()

# VLMË®≠ÂÆö
# Docker(host network) or Port Forwarding
LMDEPLOY_URL = "http://localhost:23333/v1/chat/completions"

# „É´„Éº„É´„Éô„Éº„Çπ„ÅÆ„Éï„Ç©„Éº„É´„Éê„ÉÉ„ÇØÁî®„ÉÜ„É≥„Éó„É¨„Éº„Éà
FALLBACK_TEMPLATES = {
    "Á¨ëÈ°î": [
        "ÁàΩ„ÇÑ„Åã„Å™Á¨ëÈ°î„Åå„Éñ„Éº„Çπ„ÅÆÈõ∞Âõ≤Ê∞ó„Å´„Å¥„Å£„Åü„Çä„Åß„Åó„Åü‚ú®",
        "Ëá™ÁÑ∂„Å™Á¨ëÈ°î„Åå„Å®„Å¶„ÇÇÈ≠ÖÂäõÁöÑ„Åß„Åó„Åü‚ú®",
        "Êòé„Çã„ÅÑÁ¨ëÈ°î„Åå‰ºöÂ†¥„ÇíËèØ„ÇÑ„Åã„Å´„Åó„Å¶„ÅÑ„Åæ„Åó„Åü‚ú®",
    ],
    "„ÇØ„Éº„É´": [
        "Âáõ„Å®„Åó„ÅüË°®ÊÉÖ„Åå„Å®„Å¶„ÇÇÂç∞Ë±°ÁöÑ„Åß„Åó„Åü‚ú®",
        "„ÇØ„Éº„É´„Å™Èõ∞Âõ≤Ê∞ó„Åå„Éñ„Éº„Çπ„ÅÆ‰∏ñÁïåË¶≥„Å´Âêà„Å£„Å¶„ÅÑ„Åæ„Åó„Åü‚ú®",
        "„Ç∑„É£„Éº„Éó„Å™Ë°®ÊÉÖ„ÅåÁõÆ„ÇíÂºï„Åç„Åæ„Åó„Åü‚ú®",
    ],
    "„Åã„Çè„ÅÑ„ÅÑ": [
        "Ê†û„Åï„ÇìÂèØÊÑõ„ÅÑ„ÄÅ„Åì„Çå„ÅØÂèçÂâá‚ú®",
        "Ê†û„Åï„Çì„ÅÆÂèØÊÑõ„Åï„ÄÅË¶èÊ†ºÂ§ñüì∏",
        "Ê†û„Åï„ÇìÂèØÊÑõ„Åô„Åé„Å¶ÁÑ°ÁêÜ‚ú®",
    ],
    "„Åµ„Åñ„Åë": [
        "Ê†û„Åï„Çì„ÅÆ„Åì„ÅÆ„Éé„É™„ÄÅÊúÄÈ´òüì∏",
        "Ê†û„Åï„ÇìÈù¢ÁôΩ„Åô„Åé„Çã‚ú®",
        "Ê†û„Åï„Çì„Åì„ÇåÂ•Ω„Åç„ÄÅÂÑ™Âãùüì∏",
    ],
    "ÁúüÂâ£": [
        "Ê†û„Åï„Çì„ÅÆÁúüÂâ£„Å™ÁúºÂ∑Æ„Åó„ÄÅÂà∫„Åï„Çã‚ú®",
        "Ê†û„Åï„Çì„Åì„ÅÆË°®ÊÉÖ„ÄÅÁæé„Åó„ÅÑüì∏",
        "Ê†û„Åï„Çì„ÅÆÈõÜ‰∏≠Âäõ„ÄÅ„É§„Éê„ÅÑ‚ú®",
    ],
}

def generate_fallback_comment(expression_type: str = "Á¨ëÈ°î", name: str = "") -> str:
    """„É´„Éº„É´„Éô„Éº„Çπ„Åß„Éï„Ç©„Éº„É´„Éê„ÉÉ„ÇØ„Ç≥„É°„É≥„Éà„ÇíÁîüÊàê"""
    templates = FALLBACK_TEMPLATES.get(expression_type, FALLBACK_TEMPLATES["Á¨ëÈ°î"])
    comment = random.choice(templates)

    # ÂêçÂâç„Åå„ÅÇ„Çå„Å∞ÁΩÆÊèõÔºà„ÉÜ„É≥„Éó„É¨„Éº„Éà„Å´„Çà„Å£„Å¶„ÅØÂêçÂâç„ÅåÂÖ•„Çâ„Å™„ÅÑ„ÇÇ„ÅÆ„ÇÇ„ÅÇ„Çã„ÅÆ„Åß„ÄÅÊé•È†≠Ëæû„Å®„Åó„Å¶ËøΩÂä†„Åô„ÇãÁ∞°Êòì„É≠„Ç∏„ÉÉ„ÇØ„ÇÇÊ§úË®éÔºâ
    if name.strip():
        # "Ê†û"„ÅåÂê´„Åæ„Çå„Å¶„ÅÑ„Çå„Å∞ÁΩÆÊèõ
        if "Ê†û" in comment:
            comment = comment.replace("Ê†û", name)
        # Âê´„Åæ„Çå„Å¶„ÅÑ„Å™„Åë„Çå„Å∞Êé•È†≠Ëæû„Å®„Åó„Å¶ËøΩÂä†Ôºà„Åü„Å†„Åó„ÄÅÊñáËÑà„Å´„Çà„ÇãÔºâ
        elif "‚ú®" in comment: # Êó¢Â≠ò„ÅÆ„ÉÜ„É≥„Éó„É¨„Éº„Éà„ÅØ‚ú®„ÅßÁµÇ„Çè„Çã„ÇÇ„ÅÆ„ÅåÂ§ö„ÅÑ
             pass # „Åù„ÅÆ„Åæ„Åæ

    return comment

async def call_gemini_analysis(image_path: str, name: str = "„É¢„Éá„É´") -> Dict[str, Any]:
    """Gemini API„Çí‰ΩøÁî®„Åó„Å¶ÁîªÂÉèÂàÜÊûê„Å®„Ç≥„É°„É≥„ÉàÁîüÊàê„ÇíË°å„ÅÜ"""
    if not GEMINI_API_KEY:
        raise ValueError("GEMINI_API_KEY is not set")

    try:
        model = genai.GenerativeModel('gemini-2.0-flash-exp')

        with open(image_path, "rb") as f:
            image_data = f.read()

        prompt = f"""
        „ÅÇ„Å™„Åü„ÅØ„Ç§„Éô„É≥„ÉàÂÜôÁúü„ÅÆ„Éó„É≠„ÅÆ„Ç≠„É£„Éó„Ç∑„Éß„É≥„É©„Ç§„Çø„Éº„Åß„Åô„ÄÇ
        „Åì„ÅÆÂÜôÁúü„ÅÆ‰∫∫Áâ©ÔºàÂêçÂâç: {name}Ôºâ„Å´„Å§„ÅÑ„Å¶„ÄÅSNS„Å´ÊäïÁ®ø„Åô„Çã„Åü„ÇÅ„ÅÆÁü≠„ÅÑ„Ç≥„É°„É≥„ÉàÔºà18ÊñáÂ≠ó‰ª•ÂÜÖÔºâ„Çí3„Å§ÊèêÊ°à„Åó„Å¶„Åè„Å†„Åï„ÅÑ„ÄÇ

        Êù°‰ª∂:
        1. ÂêçÂâçÔºà{name}Ôºâ„ÇíÂê´„ÇÅ„Çã„Åì„Å®„ÄÇ
        2. ÂÜôÁúü„ÅÆË°®ÊÉÖ„ÇÑÈõ∞Âõ≤Ê∞ó„ÇíÂèçÊò†„Åô„Çã„Åì„Å®„ÄÇ
        3. „Éù„Ç∏„ÉÜ„Ç£„Éñ„ÅßÈ≠ÖÂäõÁöÑ„Å™ÂÜÖÂÆπ„Å´„Åô„Çã„Åì„Å®„ÄÇ
        4. „Çπ„É©„É≥„Ç∞Á¶ÅÊ≠¢ÔºàÁ•û„ÄÅÂÑ™Âãù„ÄÅ„Éê„ÉÅ„Éê„ÉÅÁ≠âÔºâ„ÄÇ
        5. ÊñáÊú´„Å´‚ú®„Çí„Å§„Åë„Çã„Åì„Å®„ÄÇ

        Âá∫ÂäõÂΩ¢Âºè:
        JSONÂΩ¢Âºè„ÅßÂá∫Âäõ„Åó„Å¶„Åè„Å†„Åï„ÅÑ„ÄÇ
        {{
            "expression": "Á¨ëÈ°î/„ÇØ„Éº„É´/„Åã„Çè„ÅÑ„ÅÑ/etc",
            "comments": ["„Ç≥„É°„É≥„Éà1", "„Ç≥„É°„É≥„Éà2", "„Ç≥„É°„É≥„Éà3"]
        }}
        """

        response = model.generate_content([
            {"mime_type": "image/jpeg", "data": image_data},
            prompt
        ])

        if response.text:
            text = response.text.strip().replace("```json", "").replace("```", "")
            return json.loads(text)
        else:
            raise ValueError("Empty response from Gemini")

    except Exception as e:
        logger.error(f"Gemini Error: {e}")
        raise e


async def call_vlm_analysis(image_path: str) -> Dict[str, Any]:
    """VLM„Å´ÁîªÂÉè„ÇíÊäï„Åí„Å¶60È†ÖÁõÆÂà§ÂÆö(JSON)„ÇíÂèñÂæó"""
    
    # ÁîªÂÉè„Ç®„É≥„Ç≥„Éº„Éâ
    with open(image_path, "rb") as f:
        b64_img = base64.b64encode(f.read()).decode("utf-8")
        
    # „Éó„É≠„É≥„Éó„ÉàÊßãÁØâÔºàJSON SchemaÂØæÂøúÔºâ
    questions = [f"{c['id']}: {c['question']}" for c in CRITERIA]
    criteria_list = "\n".join(questions)
    
    system_prompt = """„ÅÇ„Å™„Åü„ÅØÁîªÂÉèË™çË≠òAI„Åß„Åô„ÄÇ‰ª•‰∏ã„ÅÆÂà§ÂÆöÂü∫Ê∫ñ„Å´Âü∫„Å•„Åç„ÄÅÁîªÂÉè„ÅÆÂÜÖÂÆπ„ÇíÂàÜÊûê„Åó„Å¶„Åè„Å†„Åï„ÅÑ„ÄÇ
Âá∫Âäõ„ÅØÂøÖ„ÅöJSONÂΩ¢Âºè„ÅßË°å„Å£„Å¶„Åè„Å†„Åï„ÅÑ„ÄÇ‰ΩôË®à„Å™ÊñáÁ´†„ÅØ‰∏ÄÂàáÂê´„ÇÅ„Å™„ÅÑ„Åß„Åè„Å†„Åï„ÅÑ„ÄÇ"""

    user_prompt = f"""„Åì„ÅÆÂÜôÁúü„Çí‰ª•‰∏ã„ÅÆ60È†ÖÁõÆ„ÅßÂà§ÂÆö„Åó„ÄÅÁµêÊûú„ÇíJSON„ÅßÂá∫Âäõ„Åó„Å¶„Åè„Å†„Åï„ÅÑ„ÄÇ
Ë©≤ÂΩì„Åô„ÇãÂ†¥Âêà„ÅØ1„ÄÅ„Åó„Å™„ÅÑÂ†¥Âêà„ÅØ0„Åß„Åô„ÄÇ
Á¢∫‰ø°„ÅåÊåÅ„Å¶„Å™„ÅÑÂ†¥Âêà„Åß„ÇÇ„ÄÅ„Å©„Å°„Çâ„Åã„ÇíÈÅ∏„Çì„Åß„Åè„Å†„Åï„ÅÑ„ÄÇ

--- Âà§ÂÆöÈ†ÖÁõÆ ---
{criteria_list}

--- Âá∫ÂäõÂΩ¢Âºè ---
{{
  "criteria": {{
    "A01": 1,
    "A02": 0,
    ...
  }},
  "confidence": 0.95
}}
"""

    payload = {
        "model": "Qwen/Qwen-VL-Chat-Int4",
        "messages": [
            {"role": "system", "content": system_prompt},
            {
                "role": "user", 
                "content": [
                    {"type": "text", "text": user_prompt},
                    {"type": "image_url", "image_url": {"url": f"data:image/jpeg;base64,{b64_img}"}}
                ]
            }
        ],
        "temperature": 0.1,  # Ê±∫ÂÆöË´ñÁöÑ„Å´
        "max_tokens": 1024,
        # "response_format": {"type": "json_object"} # Qwen-VL„ÅÆÂÆüË£Ö‰æùÂ≠ò„ÅÆ„Åü„ÇÅ„ÄÅ„Éó„É≠„É≥„Éó„ÉàÊåáÁ§∫„Çí„É°„Ç§„É≥„Å´„Åô„Çã
    }
    
    async with httpx.AsyncClient(timeout=60.0) as client:
        try:
            resp = await client.post(LMDEPLOY_URL, json=payload)
            resp.raise_for_status()
            result = resp.json()
            
            # „É¨„Çπ„Éù„É≥„ÇπÊßãÈÄ†„ÅÆÁ¢∫Ë™ç
            if "choices" not in result or len(result["choices"]) == 0:
                raise ValueError("Invalid VLM response format")
                
            content = result["choices"][0]["message"]["content"]
            logger.info(f"VLM Raw Response: {content[:100]}...") # „É≠„Ç∞Âá∫Âäõ
            
            # JSON„Éë„Éº„ÇπÔºàMarkdown„ÅÆ ```json ... ``` „ÇíÈô§ÂéªÔºâ
            clean_content = content.replace("```json", "").replace("```", "").strip()
            return json.loads(clean_content)
            
        except json.JSONDecodeError:
            logger.error(f"JSON Parse Error. Content: {content}")
            raise HTTPException(status_code=500, detail="VLM returned invalid JSON")
        except Exception as e:
            logger.error(f"VLM Error: {e}")
            raise HTTPException(status_code=500, detail=f"VLM Analysis Failed: {str(e)}")

@app.post("/generate")
async def generate_comment(
    image: UploadFile = File(...),
    name: str = Form(default=""),
    count: int = Form(default=3),
):
    """
    „É°„Ç§„É≥„Ç®„É≥„Éâ„Éù„Ç§„É≥„ÉàÔºö
    1. Local VLM (MiniCPM-V) -> Scorer -> Comment
    2. Fallback: Gemini API (Cloud)
    3. Fallback: Rule-based (Random)
    """
    
    # ÁîªÂÉè‰∏ÄÊôÇ‰øùÂ≠ò
    with tempfile.NamedTemporaryFile(delete=False, suffix=".jpg") as tmp:
        content = await image.read()
        tmp.write(content)
        tmp_path = tmp.name
        
    try:
        # --- Plan A: Local VLM ---
        try:
            logger.info("Attempting Local VLM Analysis...")
            vlm_result = await call_vlm_analysis(tmp_path)
            criteria_answers = vlm_result.get("criteria", {})
            
            # 0/1 „Çí bool „Å´Â§âÊèõ
            answers_bool = {k: bool(v) for k, v in criteria_answers.items()}
            
            # „Çπ„Ç≥„Ç¢„É™„É≥„Ç∞ÔºÜ„Éë„Çø„Éº„É≥Ê±∫ÂÆö
            pattern_id, p_scores, s_scores = scorer.score_from_answers(answers_bool)
            pattern_info = scorer.patterns[pattern_id]

            # „Ç≥„É°„É≥„ÉàÁîüÊàê
            comments = []
            for _ in range(count):
                raw_comment = scorer.get_comment(pattern_id)
                # ÂêçÂâçÂÖ•„Çå
                if name.strip() and not raw_comment.startswith(name):
                    final_comment = f"{name}„Åï„Çì„ÄÅ{raw_comment}"
                else:
                    final_comment = raw_comment
                comments.append(final_comment)

            expression_str = f"{pattern_info['name']} ({pattern_info['trigger']})"

            return {
                "success": True,
                "source": "local_vlm",
                "pattern": {
                    "id": pattern_id,
                    "name": pattern_info["name"],
                    "trigger": pattern_info["trigger"]
                },
                "expression": expression_str,
                "analysis": expression_str,
                "scores": p_scores,
                "comments": comments,
                "analysis_raw": vlm_result
            }

        except (httpx.ConnectError, httpx.HTTPError) as e:
            logger.warning(f"Local VLM failed ({e}), switching to Plan B (Gemini)...")
            raise ValueError("Local VLM Unavailable") # Trigger Plan B

    except ValueError:
        # --- Plan B: Gemini API ---
        try:
            if not GEMINI_API_KEY:
                logger.warning("Gemini API Key not found, switching to Plan C (Rule-based)...")
                raise ValueError("No API Key")

            logger.info("Attempting Gemini Analysis...")
            gemini_result = await call_gemini_analysis(tmp_path, name)

            return {
                "success": True,
                "source": "gemini_cloud",
                "expression": gemini_result.get("expression", "Unknown"),
                "comments": gemini_result.get("comments", []),
                "analysis": gemini_result.get("expression", "Unknown")
            }

        except Exception as e:
            logger.error(f"Gemini failed ({e}), switching to Plan C (Rule-based)...")

            # --- Plan C: Rule-based Fallback ---
            comments = []
            for _ in range(count):
                comments.append(generate_fallback_comment("Á¨ëÈ°î", name)) # „Éá„Éï„Ç©„É´„ÉàÁ¨ëÈ°î

            return {
                "success": True,
                "source": "rule_based_fallback",
                "expression": "Fallback (Rule-based)",
                "comments": comments,
                "analysis": "Fallback"
            }
        
    finally:
        if os.path.exists(tmp_path):
            os.unlink(tmp_path)

if __name__ == "__main__":
    print("\nüêØ Kotaro-Engine API Server (V2.3)")
    print(f"Connecting to VLM: {LMDEPLOY_URL}")
    print("=" * 40)
    uvicorn.run(app, host="0.0.0.0", port=8000)
