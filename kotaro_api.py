"""
Kotaro-Engine API Server (V2.3 LMDeploy Edition + Gemini Hybrid)
"""
from fastapi import FastAPI, File, UploadFile, Form, HTTPException, Body
from fastapi.middleware.cors import CORSMiddleware
from pydantic import BaseModel
import httpx
import uvicorn
import base64
import tempfile
import os
import json
import logging
from typing import List, Dict, Any, Optional
from dotenv import load_dotenv

# ç’°å¢ƒå¤‰æ•°èª­ã¿è¾¼ã¿
load_dotenv()

# Geminiã®ã‚¤ãƒ³ãƒãƒ¼ãƒˆ
import google.generativeai as genai

# Kotaroå†…éƒ¨ãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«
from kotaro_scoring import KotaroScorer, CRITERIA

# ãƒ­ã‚¬ãƒ¼è¨­å®š
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger("kotaro_api")

app = FastAPI(title="Kotaro-Engine API (V2.3 Hybrid)")

# CORS
app.add_middleware(
    CORSMiddleware,
    allow_origins=["*"], # é–‹ç™ºç”¨: Next.jsç­‰ã‹ã‚‰ã®ã‚¢ã‚¯ã‚»ã‚¹ã‚’è¨±å¯
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)

# -----------------------------------------------------------------------------
# Configuration
# -----------------------------------------------------------------------------

# VLMè¨­å®š (Local)
# Docker(host network) or Port Forwarding
LMDEPLOY_URL = "http://localhost:23333/v1/chat/completions"

# Gemini APIè¨­å®š (Cloud)
GEMINI_API_KEY = os.getenv("GEMINI_API_KEY")
if GEMINI_API_KEY:
    genai.configure(api_key=GEMINI_API_KEY)

# ã‚¹ã‚³ã‚¢ãƒ©ãƒ¼åˆæœŸåŒ–
scorer = KotaroScorer()

# -----------------------------------------------------------------------------
# Data Models (from api/main.py)
# -----------------------------------------------------------------------------

class CommentRequest(BaseModel):
    """Gemini APIç”¨ãƒªã‚¯ã‚¨ã‚¹ãƒˆãƒ¢ãƒ‡ãƒ«"""
    booth_name: str = "ãƒ–ãƒ¼ã‚¹"
    role: str = "ãƒ¢ãƒ‡ãƒ«"
    category: str = "ãƒ–ãƒ¼ã‚¹"
    expression_type: str = "ç¬‘é¡”"
    focus_point: str = "è¡¨æƒ…"
    context_match: str = "ãƒ–ãƒ¼ã‚¹ã®é›°å›²æ°—"
    image_base64: Optional[str] = None

class CommentResponse(BaseModel):
    """Gemini APIç”¨ãƒ¬ã‚¹ãƒãƒ³ã‚¹ãƒ¢ãƒ‡ãƒ«"""
    comment: str
    source: str  # "ai" or "rule_based"

class HealthResponse(BaseModel):
    status: str
    api_configured: bool
    local_vlm: bool

# -----------------------------------------------------------------------------
# Helper Functions (Local VLM)
# -----------------------------------------------------------------------------

async def call_vlm_analysis(image_path: str) -> Dict[str, Any]:
    """VLMã«ç”»åƒã‚’æŠ•ã’ã¦60é …ç›®åˆ¤å®š(JSON)ã‚’å–å¾—"""
    
    # ç”»åƒã‚¨ãƒ³ã‚³ãƒ¼ãƒ‰
    with open(image_path, "rb") as f:
        b64_img = base64.b64encode(f.read()).decode("utf-8")
        
    # ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆæ§‹ç¯‰ï¼ˆJSON Schemaå¯¾å¿œï¼‰
    questions = [f"{c['id']}: {c['question']}" for c in CRITERIA]
    criteria_list = "\n".join(questions)
    
    system_prompt = """ã‚ãªãŸã¯ç”»åƒèªè­˜AIã§ã™ã€‚ä»¥ä¸‹ã®åˆ¤å®šåŸºæº–ã«åŸºã¥ãã€ç”»åƒã®å†…å®¹ã‚’åˆ†æã—ã¦ãã ã•ã„ã€‚
å‡ºåŠ›ã¯å¿…ãšJSONå½¢å¼ã§è¡Œã£ã¦ãã ã•ã„ã€‚ä½™è¨ˆãªæ–‡ç« ã¯ä¸€åˆ‡å«ã‚ãªã„ã§ãã ã•ã„ã€‚"""

    user_prompt = f"""ã“ã®å†™çœŸã‚’ä»¥ä¸‹ã®60é …ç›®ã§åˆ¤å®šã—ã€çµæœã‚’JSONã§å‡ºåŠ›ã—ã¦ãã ã•ã„ã€‚
è©²å½“ã™ã‚‹å ´åˆã¯1ã€ã—ãªã„å ´åˆã¯0ã§ã™ã€‚
ç¢ºä¿¡ãŒæŒã¦ãªã„å ´åˆã§ã‚‚ã€ã©ã¡ã‚‰ã‹ã‚’é¸ã‚“ã§ãã ã•ã„ã€‚

--- åˆ¤å®šé …ç›® ---
{criteria_list}

--- å‡ºåŠ›å½¢å¼ ---
{{
  "criteria": {{
    "A01": 1,
    "A02": 0,
    ...
  }},
  "confidence": 0.95
}}
"""

    payload = {
        "model": "Qwen/Qwen-VL-Chat-Int4",
        "messages": [
            {"role": "system", "content": system_prompt},
            {
                "role": "user", 
                "content": [
                    {"type": "text", "text": user_prompt},
                    {"type": "image_url", "image_url": {"url": f"data:image/jpeg;base64,{b64_img}"}}
                ]
            }
        ],
        "temperature": 0.1,  # æ±ºå®šè«–çš„ã«
        "max_tokens": 1024,
    }
    
    async with httpx.AsyncClient(timeout=60.0) as client:
        try:
            resp = await client.post(LMDEPLOY_URL, json=payload)
            resp.raise_for_status()
            result = resp.json()
            
            # ãƒ¬ã‚¹ãƒãƒ³ã‚¹æ§‹é€ ã®ç¢ºèª
            if "choices" not in result or len(result["choices"]) == 0:
                raise ValueError("Invalid VLM response format")
                
            content = result["choices"][0]["message"]["content"]
            logger.info(f"VLM Raw Response: {content[:100]}...") # ãƒ­ã‚°å‡ºåŠ›
            
            # JSONãƒ‘ãƒ¼ã‚¹ï¼ˆMarkdownã® ```json ... ``` ã‚’é™¤å»ï¼‰
            clean_content = content.replace("```json", "").replace("```", "").strip()
            return json.loads(clean_content)
            
        except json.JSONDecodeError:
            logger.error(f"JSON Parse Error. Content: {content}")
            raise HTTPException(status_code=500, detail="VLM returned invalid JSON")
        except Exception as e:
            logger.error(f"VLM Error: {e}")
            raise HTTPException(status_code=500, detail=f"VLM Analysis Failed: {str(e)}")

# -----------------------------------------------------------------------------
# Helper Functions (Gemini / Rule Based)
# -----------------------------------------------------------------------------

FALLBACK_TEMPLATES = {
    "ç¬‘é¡”": [
        "çˆ½ã‚„ã‹ãªç¬‘é¡”ãŒãƒ–ãƒ¼ã‚¹ã®é›°å›²æ°—ã«ã´ã£ãŸã‚Šã§ã—ãŸâœ¨",
        "è‡ªç„¶ãªç¬‘é¡”ãŒã¨ã¦ã‚‚é­…åŠ›çš„ã§ã—ãŸâœ¨",
        "æ˜ã‚‹ã„ç¬‘é¡”ãŒä¼šå ´ã‚’è¯ã‚„ã‹ã«ã—ã¦ã„ã¾ã—ãŸâœ¨",
    ],
    "ã‚¯ãƒ¼ãƒ«": [
        "å‡›ã¨ã—ãŸè¡¨æƒ…ãŒã¨ã¦ã‚‚å°è±¡çš„ã§ã—ãŸâœ¨",
        "ã‚¯ãƒ¼ãƒ«ãªé›°å›²æ°—ãŒãƒ–ãƒ¼ã‚¹ã®ä¸–ç•Œè¦³ã«åˆã£ã¦ã„ã¾ã—ãŸâœ¨",
        "ã‚·ãƒ£ãƒ¼ãƒ—ãªè¡¨æƒ…ãŒç›®ã‚’å¼•ãã¾ã—ãŸâœ¨",
    ],
    # ... ä»–ã®ãƒ†ãƒ³ãƒ—ãƒ¬ãƒ¼ãƒˆã¯å¿…è¦ã«å¿œã˜ã¦è¿½åŠ  ...
}

def generate_fallback_comment(expression_type: str) -> str:
    """ãƒ«ãƒ¼ãƒ«ãƒ™ãƒ¼ã‚¹ã§ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯ã‚³ãƒ¡ãƒ³ãƒˆã‚’ç”Ÿæˆ"""
    import random
    templates = FALLBACK_TEMPLATES.get(expression_type, FALLBACK_TEMPLATES["ç¬‘é¡”"])
    return random.choice(templates)

def build_gemini_prompt(request: CommentRequest, has_image: bool) -> str:
    """Geminiç”¨ã®ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã‚’æ§‹ç¯‰"""
    prompt = f"""ã‚ãªãŸã¯ã‚¤ãƒ™ãƒ³ãƒˆå†™çœŸã®ä¸€è¨€ã‚³ãƒ¡ãƒ³ãƒˆã‚’æ›¸ããƒ—ãƒ­ã§ã™ã€‚
{'ã“ã®å†™çœŸã‚’è¦‹ã¦ã€' if has_image else ''}ä»¥ä¸‹ã®ãƒ«ãƒ¼ãƒ«ã§1è¡Œã‚³ãƒ¡ãƒ³ãƒˆã‚’1ã¤ã ã‘ç”Ÿæˆã—ã¦ãã ã•ã„ï¼š

ã€çµ¶å¯¾ãƒ«ãƒ¼ãƒ«ã€‘
- 1è¡Œã®ã¿ï¼ˆ20ã€œ30æ–‡å­—ï¼‰
- ã€Œã€‡ã€‡ãŒâ–³â–³ã«ã´ã£ãŸã‚Š/åˆã£ã¦ã„ãŸã€å½¢å¼
- æœ€å¾Œã«âœ¨ã‚’ä»˜ã‘ã‚‹
- å›ºæœ‰åè©ãƒ»ã‚­ãƒ£ãƒ©åãƒ»ä½œå“åã¯çµ¶å¯¾ã«å…¥ã‚Œãªã„
- ä¸»èªã‚’ã€Œä¿ºã€ã«ã—ãªã„
- ã‚¹ãƒ©ãƒ³ã‚°ç¦æ­¢ï¼ˆç¥ã€å„ªå‹ã€ãƒãƒãƒãƒç­‰ï¼‰

ã€ä½¿ãˆã‚‹è©•ä¾¡è»¸ã®ã¿ä½¿ç”¨ã€‘
ç¬‘é¡”ã€è¡¨æƒ…ã€è¦–ç·šã€ä½‡ã¾ã„ã€é›°å›²æ°—ã€è¡£è£…ãŒä¼¼åˆã†ã€ãƒ©ã‚¤ãƒˆã«æ˜ ãˆã‚‹ã€ãƒ–ãƒ¼ã‚¹ã®é›°å›²æ°—ã«åˆã†

{'ã€å†™çœŸã‹ã‚‰èª­ã¿å–ã‚‹ã¹ãè¦ç´ ã€‘' if has_image else ''}
{'''- äººç‰©ã®è¡¨æƒ…ï¼ˆç¬‘é¡”ã€ã‚¯ãƒ¼ãƒ«ã€å„ªã—ã„ã€å‡›ã¨ã—ãŸãªã©ï¼‰
- å…¨ä½“ã®é›°å›²æ°—ï¼ˆæ˜ã‚‹ã„ã€è½ã¡ç€ã„ãŸã€è¯ã‚„ã‹ãªã©ï¼‰
- è¡£è£…ã‚„ãƒ©ã‚¤ãƒ†ã‚£ãƒ³ã‚°ã®å°è±¡''' if has_image else ''}

ã€ãƒ¦ãƒ¼ã‚¶ãƒ¼ãŒé¸æŠã—ãŸé›°å›²æ°—ã€‘
- è¡¨æƒ…ãƒ»é›°å›²æ°—: {request.expression_type}
- æ³¨ç›®ãƒã‚¤ãƒ³ãƒˆ: {request.focus_point}
- ãƒãƒƒãƒå…ˆ: {request.context_match}

ã€æƒ…å ±ã€‘
- ã‚«ãƒ†ã‚´ãƒª: {request.category}
- ãƒ–ãƒ¼ã‚¹: {request.booth_name}
- å½¹å‰²: {request.role}

ã€å‡ºåŠ›å½¢å¼ã€‘
ã‚³ãƒ¡ãƒ³ãƒˆã®ã¿ã‚’1è¡Œã§å‡ºåŠ›ï¼ˆèª¬æ˜ä¸è¦ï¼‰"""
    return prompt

# -----------------------------------------------------------------------------
# Endpoints
# -----------------------------------------------------------------------------

@app.get("/health", response_model=HealthResponse)
async def health_check():
    """çµ±åˆãƒ˜ãƒ«ã‚¹ãƒã‚§ãƒƒã‚¯"""
    # Local VLM check (simple connection check)
    local_vlm_status = False
    try:
        async with httpx.AsyncClient(timeout=2.0) as client:
             # VLM health check or just connect to root/models
             # LMDeploy usually has /v1/models
             resp = await client.get(LMDEPLOY_URL.replace("/chat/completions", "/models"))
             if resp.status_code == 200:
                 local_vlm_status = True
    except:
        pass

    return HealthResponse(
        status="ok",
        api_configured=bool(GEMINI_API_KEY),
        local_vlm=local_vlm_status
    )

@app.post("/generate")
async def generate_comment_local(
    image: UploadFile = File(...),
    name: str = Form(default=""),
    count: int = Form(default=3),
):
    """
    [Local Mode] ç”»åƒ -> VLM -> Scorer -> Comment
    Next.js App (Candy Kotaro) ç”¨ã®ãƒ¡ã‚¤ãƒ³ã‚¨ãƒ³ãƒ‰ãƒã‚¤ãƒ³ãƒˆ
    """
    
    # ç”»åƒä¸€æ™‚ä¿å­˜
    with tempfile.NamedTemporaryFile(delete=False, suffix=".jpg") as tmp:
        content = await image.read()
        tmp.write(content)
        tmp_path = tmp.name
        
    try:
        # 1. VLMåˆ†æï¼ˆ60é …ç›®åˆ¤å®šï¼‰
        logger.info("Calling VLM...")
        vlm_result = await call_vlm_analysis(tmp_path)
        criteria_answers = vlm_result.get("criteria", {})
        
        # 0/1 ã‚’ bool ã«å¤‰æ›
        answers_bool = {k: bool(v) for k, v in criteria_answers.items()}
        
        # 2. ã‚¹ã‚³ã‚¢ãƒªãƒ³ã‚°ï¼†ãƒ‘ã‚¿ãƒ¼ãƒ³æ±ºå®š
        logger.info("Scoring...")
        pattern_id, p_scores, s_scores = scorer.score_from_answers(answers_bool)
        pattern_info = scorer.patterns[pattern_id]
        
        # 3. ã‚³ãƒ¡ãƒ³ãƒˆç”Ÿæˆ
        comments = []
        for _ in range(count):
            raw_comment = scorer.get_comment(pattern_id)
            # åå‰å…¥ã‚Œ
            if name.strip() and not raw_comment.startswith(name):
                final_comment = f"{name}ã•ã‚“ã€{raw_comment}"
            else:
                final_comment = raw_comment
            comments.append(final_comment)
            
        # å¾Œæ–¹äº’æ›æ€§ãƒ•ã‚£ãƒ¼ãƒ«ãƒ‰
        expression_str = f"{pattern_info['name']} ({pattern_info['trigger']})"
            
        return {
            "success": True,
            "pattern": {
                "id": pattern_id,
                "name": pattern_info["name"],
                "trigger": pattern_info["trigger"]
            },
            "expression": expression_str, # ãƒ•ãƒ­ãƒ³ãƒˆã‚¨ãƒ³ãƒ‰äº’æ›æ€§
            "analysis": expression_str,   # æ—§APIäº’æ›æ€§
            "scores": p_scores,
            "comments": comments,
            "analysis_raw": vlm_result # ãƒ•ãƒ­ãƒ³ãƒˆã‚¨ãƒ³ãƒ‰ã§ã®ãƒ‡ãƒãƒƒã‚°ç”¨ã«ç¶­æŒ
        }
        
    except httpx.ConnectError:
        logger.error("VLM Connection Failed")
        return {
            "success": False,
            "error": "VLM engine is offline. Please start LMDeploy container."
        }
    except Exception as e:
        logger.error(f"API Error: {e}")
        return {"success": False, "error": str(e)}
        
    finally:
        if os.path.exists(tmp_path):
            os.unlink(tmp_path)

@app.post("/generate-comment", response_model=CommentResponse)
async def generate_comment_cloud(request: CommentRequest):
    """
    [Cloud Mode] Gemini APIã‚’ä½¿ç”¨ã—ãŸä¸€è¨€ã‚³ãƒ¡ãƒ³ãƒˆç”Ÿæˆ
    api/main.py ã‹ã‚‰ã®ç§»è¡Œ
    """

    if not GEMINI_API_KEY:
        # APIæœªè¨­å®šã®å ´åˆã¯ãƒ«ãƒ¼ãƒ«ãƒ™ãƒ¼ã‚¹ã§ç”Ÿæˆ
        comment = generate_fallback_comment(request.expression_type)
        return CommentResponse(comment=comment, source="rule_based")

    try:
        # Geminiãƒ¢ãƒ‡ãƒ«ã‚’åˆæœŸåŒ–
        model = genai.GenerativeModel('gemini-2.0-flash-exp')

        # ç”»åƒã®æœ‰ç„¡ã‚’ç¢ºèª
        has_image = bool(request.image_base64)

        # ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã‚’æ§‹ç¯‰
        prompt = build_gemini_prompt(request, has_image)

        if has_image:
            # ç”»åƒä»˜ããƒªã‚¯ã‚¨ã‚¹ãƒˆ
            if request.image_base64.startswith('data:'):
                header, image_data = request.image_base64.split(',', 1)
                mime_type = header.split(':')[1].split(';')[0]
            else:
                image_data = request.image_base64
                mime_type = "image/jpeg"

            image_bytes = base64.b64decode(image_data)

            response = model.generate_content([
                {
                    "mime_type": mime_type,
                    "data": image_bytes
                },
                prompt
            ])
        else:
            response = model.generate_content(prompt)

        if response.text:
            comment = response.text.strip()
            return CommentResponse(comment=comment, source="ai")
        else:
            raise ValueError("Empty response from API")

    except Exception as e:
        logger.error(f"Gemini API error: {e}")
        comment = generate_fallback_comment(request.expression_type)
        return CommentResponse(comment=comment, source="rule_based")

@app.get("/")
async def root():
    """ãƒ«ãƒ¼ãƒˆã‚¨ãƒ³ãƒ‰ãƒã‚¤ãƒ³ãƒˆ"""
    return {
        "message": "Kotaro-Engine API (Hybrid)",
        "modes": {
            "local": "/generate (POST, FormData)",
            "cloud": "/generate-comment (POST, JSON)"
        },
        "docs": "/docs",
        "health": "/health"
    }

if __name__ == "__main__":
    print("\nğŸ¯ Kotaro-Engine API Server (V2.3 Hybrid)")
    print(f"Connecting to VLM: {LMDEPLOY_URL}")
    print(f"Gemini API: {'Configured' if GEMINI_API_KEY else 'Not Configured'}")
    print("=" * 40)
    uvicorn.run(app, host="0.0.0.0", port=8000)
