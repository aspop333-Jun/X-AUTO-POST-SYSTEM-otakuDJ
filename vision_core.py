"""
Vision Core: MiniCPM-V 2.6 int4 ç”»åƒè§£æãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«
kotarou-engine è¦–è¦šèªè­˜ã‚³ã‚¢

RTX 4060 (8GB VRAM) æœ€é©åŒ–æ¸ˆã¿
- 4-bité‡å­åŒ– (ç´„7GB VRAMä½¿ç”¨)
- ç”»åƒãƒªã‚µã‚¤ã‚º (512px) ã«ã‚ˆã‚‹VRAMç¯€ç´„
- æ¨è«–å¾ŒVRAMè§£æ”¾

ä½¿ç”¨æ–¹æ³•:
    from vision_core import VisionCore
    
    vision = VisionCore()
    result = vision.analyze("path/to/image.jpg")
"""

import torch
from PIL import Image
from pathlib import Path
from typing import Optional, Dict, Any
import logging

# ãƒ­ã‚°è¨­å®š
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)


# =============================================================================
# è¨­å®šå€¤ï¼ˆæŒ‡ä»¤æ›¸æº–æ‹ ï¼‰
# =============================================================================

CONFIG = {
    # ãƒ¢ãƒ‡ãƒ«è¨­å®š
    "model_id": "openbmb/MiniCPM-V-2_6-int4",
    
    # ç”»åƒå‰å‡¦ç†
    "max_image_size": 512,  # é•·è¾ºæœ€å¤§pxï¼ˆVRAMç¯€ç´„ï¼‰
    
    # ãƒ‡ã‚³ãƒ¼ãƒ‡ã‚£ãƒ³ã‚°è¨­å®šï¼ˆãƒãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³æŠ‘åˆ¶ï¼‰
    "temperature": 0.3,      # ä½æ¸©åº¦ â†’ å¿ å®Ÿåº¦å‘ä¸Š
    "top_p": 0.8,            # ç¢ºä¿¡åº¦ä½ã„æƒ…å ±ã®æ’é™¤
    "repetition_penalty": 1.2,  # ãƒ«ãƒ¼ãƒ—é˜²æ­¢
    "max_new_tokens": 512,
}

# ã‚·ã‚¹ãƒ†ãƒ ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆï¼ˆæŒ‡ä»¤æ›¸æº–æ‹ ï¼‰
SYSTEM_PROMPT = """ã‚ãªãŸã¯ãƒ—ãƒ­ã®ãƒ•ã‚©ãƒˆã‚°ãƒ©ãƒ•ã‚¡ãƒ¼å…¼ã€å„ªã‚ŒãŸè¦³å¯Ÿçœ¼ã‚’æŒã¤ãƒ©ã‚¤ã‚¿ãƒ¼ã§ã™ã€‚
å…¥åŠ›ã•ã‚ŒãŸç”»åƒã«å¯¾ã—ã€ä»¥ä¸‹ã®4é …ç›®ã‚’è©³ç´°ã«ã€ã‹ã¤å®¢è¦³çš„ã«æ—¥æœ¬èªã§åˆ†æã—ã¦ãã ã•ã„ã€‚

ã€é‡è¦ãƒ«ãƒ¼ãƒ«ã€‘
- æ¨æ¸¬ã®ç¦æ­¢: å†™ã£ã¦ã„ãªã„ã‚‚ã®ã‚’ã€ŒãŠãã‚‰ãã€œã ã‚ã†ã€ã¨è¨˜è¿°ã™ã‚‹ã“ã¨ã‚’å³ç¦ã¨ã™ã‚‹
- ä½ç…§åº¦ãƒ»ééœ²å‡ºã¸ã®å¯¾å¿œ: ç”»åƒãŒçœ©ã—ã™ãã‚‹/æš—ã™ãã‚‹å ´åˆã€ãã®çŠ¶æ…‹è‡ªä½“ã‚’ã€Œå…‰ã«åŒ…ã¾ã‚ŒãŸã€ã€Œå½±ã«æ²ˆã‚“ã ã€ã¨äº‹å®Ÿã¨ã—ã¦è¨˜è¿°ã™ã‚‹
- å‡ºåŠ›ã¯ç®‡æ¡æ›¸ãã§è¨˜è¿°ã™ã‚‹"""

USER_PROMPT = """ã“ã®å†™çœŸã‚’ä»¥ä¸‹ã®4é …ç›®ã§åˆ†æã—ã¦ãã ã•ã„ï¼š

â–  ä¸»å½¹ã®è¦ç´ 
äººç‰©ã®è¡¨æƒ…ã€ãƒãƒ¼ã‚ºã€è¦–ç·šã€è¡£è£…ã®è©³ç´°ã‚’è¨˜è¿°

â–  å…‰ã¨è‰²ã®ç©ºæ°—æ„Ÿ
å…‰ã®å·®ã—æ–¹ï¼ˆé€†å…‰ã€ã‚µã‚¤ãƒ‰å…‰ãªã©ï¼‰ã€è‰²æ¸©åº¦ã€å…¨ä½“ã®ãƒˆãƒ¼ãƒ³ã‚’è¨˜è¿°

â–  èƒŒæ™¯ã¨ã‚·ãƒãƒ¥ã‚¨ãƒ¼ã‚·ãƒ§ãƒ³
å ´æ‰€ã®ç‰¹å®šã€å­£ç¯€æ„Ÿã€å‘¨å›²ã®ã‚ªãƒ–ã‚¸ã‚§ã‚¯ãƒˆã‚’è¨˜è¿°

â–  ã‚¨ãƒ¢ãƒ¼ã‚·ãƒ§ãƒŠãƒ«ãƒ»ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰
ç”»åƒã‹ã‚‰æ„Ÿã˜å–ã‚Œã‚‹ã€Œåˆ‡ãªã•ã€ã€Œå¸Œæœ›ã€ã€Œé™å¯‚ã€ãªã©ã®æŠ½è±¡çš„ãªã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã‚’3ã¤ç¨‹åº¦"""


# =============================================================================
# VisionCore ã‚¯ãƒ©ã‚¹
# =============================================================================

class VisionCore:
    """MiniCPM-V 2.6 int4 ã«ã‚ˆã‚‹ç”»åƒè§£æ"""
    
    def __init__(self, model_id: Optional[str] = None):
        """
        Args:
            model_id: HuggingFaceãƒ¢ãƒ‡ãƒ«IDï¼ˆãƒ‡ãƒ•ã‚©ãƒ«ãƒˆ: openbmb/MiniCPM-V-2_6-int4ï¼‰
        """
        self.model_id = model_id or CONFIG["model_id"]
        self.model = None
        self.tokenizer = None
        self._loaded = False
        
    def _load_model(self):
        """ãƒ¢ãƒ‡ãƒ«ã‚’é…å»¶ãƒ­ãƒ¼ãƒ‰"""
        if self._loaded:
            return
        
        # CUDAãƒã‚§ãƒƒã‚¯
        if not torch.cuda.is_available():
            raise RuntimeError("âŒ CUDA is not available! GPU is required for MiniCPM-V inference.")
        
        device_name = torch.cuda.get_device_name(0)
        logger.info(f"ğŸ® GPUæ¤œå‡º: {device_name}")
        logger.info(f"ğŸ”„ ãƒ¢ãƒ‡ãƒ«ãƒ­ãƒ¼ãƒ‰ä¸­: {self.model_id}")
        
        from transformers import AutoModel, AutoTokenizer
        
        # int4é‡å­åŒ–ç‰ˆã¯trust_remote_codeãŒå¿…é ˆ
        # device_map="cuda" ã§GPUæ¨è«–ã‚’å¼·åˆ¶
        self.model = AutoModel.from_pretrained(
            self.model_id,
            trust_remote_code=True,
            device_map="cuda",  # ğŸ”§ GPUæ¨è«–ã‚’å¼·åˆ¶
            torch_dtype=torch.float16,  # ğŸ”§ FP16ã§ãƒ¡ãƒ¢ãƒªåŠ¹ç‡åŒ–
        )
        self.model.eval()
        
        self.tokenizer = AutoTokenizer.from_pretrained(
            self.model_id,
            trust_remote_code=True,
        )
        
        self._loaded = True
        
        # ãƒ‡ãƒã‚¤ã‚¹ç¢ºèªãƒ­ã‚°
        model_device = next(self.model.parameters()).device
        logger.info(f"âœ… ãƒ¢ãƒ‡ãƒ«ãƒ­ãƒ¼ãƒ‰å®Œäº† (ãƒ‡ãƒã‚¤ã‚¹: {model_device})")
        
        # VRAMä½¿ç”¨é‡ãƒ­ã‚°
        vram_gb = torch.cuda.memory_allocated() / (1024 ** 3)
        vram_total = torch.cuda.get_device_properties(0).total_memory / (1024 ** 3)
        logger.info(f"ğŸ“Š VRAMä½¿ç”¨é‡: {vram_gb:.2f} GB / {vram_total:.1f} GB")
    
    def _preprocess_image(self, image_path: str) -> Image.Image:
        """ç”»åƒã®å‰å‡¦ç†ï¼ˆãƒªã‚µã‚¤ã‚ºã§VRAMç¯€ç´„ï¼‰"""
        image = Image.open(image_path).convert("RGB")
        
        # é•·è¾ºã‚’max_image_sizeã«åˆ¶é™
        max_size = CONFIG["max_image_size"]
        w, h = image.size
        
        if max(w, h) > max_size:
            if w > h:
                new_w = max_size
                new_h = int(h * max_size / w)
            else:
                new_h = max_size
                new_w = int(w * max_size / h)
            
            image = image.resize((new_w, new_h), Image.Resampling.LANCZOS)
            logger.info(f"ğŸ“ ç”»åƒãƒªã‚µã‚¤ã‚º: {w}x{h} â†’ {new_w}x{new_h}")
        
        return image
    
    def analyze(self, image_path: str) -> str:
        """
        ç”»åƒã‚’è§£æã—ã€4é …ç›®ã®ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿ã‚’ç”Ÿæˆ
        
        Args:
            image_path: ç”»åƒãƒ•ã‚¡ã‚¤ãƒ«ã®ãƒ‘ã‚¹
            
        Returns:
            ç®‡æ¡æ›¸ãå½¢å¼ã®è§£æçµæœ
        """
        # ãƒ¢ãƒ‡ãƒ«ã‚’é…å»¶ãƒ­ãƒ¼ãƒ‰
        self._load_model()
        
        # ç”»åƒå‰å‡¦ç†
        image = self._preprocess_image(image_path)
        
        logger.info(f"ğŸ“¸ ç”»åƒè§£æä¸­: {Path(image_path).name}")
        
        # ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸æ§‹ç¯‰
        msgs = [
            {"role": "system", "content": SYSTEM_PROMPT},
            {"role": "user", "content": [image, USER_PROMPT]},
        ]
        
        # æ¨è«–å®Ÿè¡Œ
        result = self.model.chat(
            image=None,
            msgs=msgs,
            tokenizer=self.tokenizer,
            sampling=True,
            temperature=CONFIG["temperature"],
            top_p=CONFIG["top_p"],
            repetition_penalty=CONFIG["repetition_penalty"],
            max_new_tokens=CONFIG["max_new_tokens"],
        )
        
        # VRAMè§£æ”¾
        self._clear_cache()
        
        logger.info("âœ… è§£æå®Œäº†")
        
        return result
    
    def analyze_simple(self, image_path: str) -> str:
        """
        kotaro_api.pyäº’æ›ã®ã‚·ãƒ³ãƒ—ãƒ«ãªè§£æï¼ˆè¤’ã‚è¦ç´ 3é …ç›®ï¼‰
        V2.1: è¡¨æƒ…ãƒ»ä»•è‰ãƒ»é›°å›²æ°—å„ªå…ˆã€è¡£è£…è‰²ã¯é™¤å¤–
        
        Args:
            image_path: ç”»åƒãƒ•ã‚¡ã‚¤ãƒ«ã®ãƒ‘ã‚¹
            
        Returns:
            JSONå½¢å¼ã®è¤’ã‚è¦ç´ ï¼ˆexpression, gesture, atmosphereï¼‰
        """
        self._load_model()
        image = self._preprocess_image(image_path)
        
        # V2.1ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆ: è¤’ã‚è¦ç´ ã®ã¿æŠ½å‡ºï¼ˆè¡£è£…è‰²ãƒ»èƒŒæ™¯ç¦æ­¢ï¼‰
        simple_prompt = """ã‚ãªãŸã¯äººç‰©å†™çœŸã®é­…åŠ›ã‚’è¦‹ã¤ã‘ã‚‹ãƒ—ãƒ­ã§ã™ã€‚

ã€ã‚¿ã‚¹ã‚¯ã€‘
ã“ã®å†™çœŸã®äººç‰©ã®ã€Œè¤’ã‚ãŸããªã‚‹ãƒã‚¤ãƒ³ãƒˆã€ã‚’3ã¤æŠ½å‡ºã—ã¦ãã ã•ã„ã€‚

ã€æŠ½å‡ºã™ã‚‹é …ç›®ã€‘
1. expression: è¡¨æƒ…ã®é­…åŠ›ï¼ˆä¾‹: ã¯ã«ã‹ã‚“ã ç¬‘é¡”ã€ã‚­ãƒ©ã‚­ãƒ©ã—ãŸç›®ã€å„ªã—ã„å¾®ç¬‘ã¿ï¼‰
2. gesture: ä»•è‰ãƒ»ãƒãƒ¼ã‚ºã®é­…åŠ›ï¼ˆä¾‹: å¯æ„›ã„ãƒ”ãƒ¼ã‚¹ã‚µã‚¤ãƒ³ã€å ‚ã€…ã¨ã—ãŸãƒãƒ¼ã‚ºã€ã‚»ã‚¯ã‚·ãƒ¼ãªç›®ç·šï¼‰
3. atmosphere: å…¨ä½“ã®é›°å›²æ°—ï¼ˆä¾‹: é€æ˜æ„ŸãŒã‚ã‚‹ã€ã‚ªãƒ¼ãƒ©ãŒã™ã”ã„ã€ç™’ã—ç³»ï¼‰

ã€ç¦æ­¢äº‹é …ã€‘
- èƒŒæ™¯ã®èª¬æ˜ã¯çµ¶å¯¾ã«ã—ãªã„
- è¡£è£…ã®è‰²ï¼ˆé’ç³»ã€èµ¤ç³»ã€ç™½ç³»ãªã©ï¼‰ã¯è¨€åŠã—ãªã„
- å›ºæœ‰åè©ãƒ»ãƒ–ãƒ©ãƒ³ãƒ‰åã¯ä½¿ã‚ãªã„
- è‹±èªã¯ä½¿ã‚ãªã„

ã€å‡ºåŠ›å½¢å¼ã€‘JSONå½¢å¼ã§å›ç­”
{"expression": "...", "gesture": "...", "atmosphere": "..."}

æ—¥æœ¬èªã®ã¿ã§å›ç­”ã—ã¦ãã ã•ã„ï¼š"""
        
        msgs = [
            {"role": "user", "content": [image, simple_prompt]},
        ]
        
        result = self.model.chat(
            image=None,
            msgs=msgs,
            tokenizer=self.tokenizer,
            sampling=True,
            temperature=0.2,  # V2.1: ãƒãƒ«ã‚·ãƒãƒ¼ã‚·ãƒ§ãƒ³æŠ‘åˆ¶
            top_p=0.9,        # V2.1: ç¢ºç‡è³ªé‡åˆ¶é™
            max_new_tokens=128,
        )
        
        self._clear_cache()
        
        return result
    
    def _clear_cache(self):
        """VRAMè§£æ”¾"""
        if torch.cuda.is_available():
            torch.cuda.empty_cache()
            logger.debug("ğŸ§¹ VRAM ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã‚¯ãƒªã‚¢")
    
    def unload(self):
        """ãƒ¢ãƒ‡ãƒ«ã‚’ã‚¢ãƒ³ãƒ­ãƒ¼ãƒ‰ã—ã¦VRAMã‚’å®Œå…¨è§£æ”¾"""
        if self._loaded:
            del self.model
            del self.tokenizer
            self.model = None
            self.tokenizer = None
            self._loaded = False
            
            if torch.cuda.is_available():
                torch.cuda.empty_cache()
            
            logger.info("ğŸ”Œ ãƒ¢ãƒ‡ãƒ«ã‚¢ãƒ³ãƒ­ãƒ¼ãƒ‰å®Œäº†")


# =============================================================================
# ã‚°ãƒ­ãƒ¼ãƒãƒ«ã‚¤ãƒ³ã‚¹ã‚¿ãƒ³ã‚¹ï¼ˆã‚·ãƒ³ã‚°ãƒ«ãƒˆãƒ³ï¼‰
# =============================================================================

_vision_core_instance: Optional[VisionCore] = None


def get_vision_core() -> VisionCore:
    """ã‚·ãƒ³ã‚°ãƒ«ãƒˆãƒ³ã‚¤ãƒ³ã‚¹ã‚¿ãƒ³ã‚¹ã‚’å–å¾—"""
    global _vision_core_instance
    if _vision_core_instance is None:
        _vision_core_instance = VisionCore()
    return _vision_core_instance


def analyze_image_minicpm(image_path: str) -> str:
    """
    kotaro_api.py ã‹ã‚‰ã®å‘¼ã³å‡ºã—ç”¨é–¢æ•°
    
    Args:
        image_path: ç”»åƒãƒ•ã‚¡ã‚¤ãƒ«ã®ãƒ‘ã‚¹
        
    Returns:
        ç”»åƒè§£æçµæœï¼ˆ3é …ç›®ï¼‰
    """
    vision = get_vision_core()
    return vision.analyze_simple(image_path)


# =============================================================================
# CLI ãƒ†ã‚¹ãƒˆ
# =============================================================================

if __name__ == "__main__":
    import argparse
    
    parser = argparse.ArgumentParser(description="MiniCPM-V 2.6 Vision Core")
    parser.add_argument("--image", type=str, required=True, help="ç”»åƒãƒ•ã‚¡ã‚¤ãƒ«ã®ãƒ‘ã‚¹")
    parser.add_argument("--mode", type=str, default="simple", choices=["simple", "full"],
                        help="è§£æãƒ¢ãƒ¼ãƒ‰: simple=3é …ç›®, full=4é …ç›®è©³ç´°")
    
    args = parser.parse_args()
    
    print("\n" + "=" * 60)
    print("ğŸ‘ï¸  MiniCPM-V 2.6 Vision Core")
    print("=" * 60)
    
    vision = VisionCore()
    
    if args.mode == "full":
        result = vision.analyze(args.image)
    else:
        result = vision.analyze_simple(args.image)
    
    print("\nğŸ“‹ è§£æçµæœ:")
    print("-" * 40)
    print(result)
    print("-" * 40)
    
    vision.unload()
    print("=" * 60 + "\n")
